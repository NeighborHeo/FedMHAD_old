{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_unique_classes(dataset):\n",
    "    unique_classes = set()\n",
    "    for img, mask in dataset:\n",
    "        classes = list(mask.getcolors())\n",
    "        for count, pixel_value in classes:\n",
    "            unique_classes.add(pixel_value)\n",
    "    \n",
    "    return unique_classes\n",
    "\n",
    "def display_images(dataset, num_images=5):\n",
    "    fig, ax = plt.subplots(num_images, 2, figsize=(10, num_images * 5))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # 이미지 및 마스크 가져오기\n",
    "        img, mask = dataset[i]\n",
    "        \n",
    "        img = to_pil_image(img)\n",
    "        mask = to_pil_image(mask)\n",
    "\n",
    "        # 이미지 및 마스크 표시\n",
    "        ax[i, 0].imshow(img)\n",
    "        ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "        ax[i, 1].imshow(mask, cmap='gray')\n",
    "        ax[i, 1].set_title(f\"Segmentation Image {i + 1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# 데이터셋 다운로드 및 생성\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "voc_dataset = VOCSegmentation(root='~/.data', year='2012', image_set='train', download=False, transform=transforms)\n",
    "voc_dataloader = torch.utils.data.DataLoader(voc_dataset, batch_size=10, shuffle=True)\n",
    "# 데이터셋에서 이미지 및 마스크 표시\n",
    "display_images(voc_dataset)\n",
    "print(voc_dataset[0])\n",
    "print(voc_dataset[0][1])\n",
    "# # 데이터셋에서 고유한 객체 클래스 확인\n",
    "unique_classes = get_unique_classes(voc_dataset)\n",
    "# print(f\"Unique classes in the dataset: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_classes)\n",
    "num_classes = len(unique_classes)-2\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat', 9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse', 14: 'motorbike', 15: 'person', 16: 'pottedplant', 17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor', 255: 'ambigious'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, mask in voc_dataloader:\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def one_hot_embedding(labels, num_classes=20):\n",
    "    # multi-labels\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "classes = one_hot_embedding(classes, num_classes=num_classes)\n",
    "classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import collections\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class myVOCSegmentation(VOCSegmentation):\n",
    "    def __init__(self, root, year, image_set, download=False, transform=None, target_transform=None, transforms=None):\n",
    "        super().__init__(root, year, image_set, download, transform, target_transform, transforms)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, segmentation_mask = super().__getitem__(index)\n",
    "        \n",
    "        find_classes = self.find_classes(segmentation_mask)\n",
    "\n",
    "        return img, target, segmentation_mask\n",
    "\n",
    "    def find_classes(self, segmentation_mask):\n",
    "        return torch.tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResizeAndTransformBoth:\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, target, segmentation_mask):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        segmentation_mask = segmentation_mask.resize(self.size, self.interpolation)\n",
    "        return img, target, segmentation_mask\n",
    "    \n",
    "# 데이터셋을 정의하고 다운로드 받는 부분 (예시)\n",
    "root = \"data/\"\n",
    "year = \"2012\"\n",
    "image_set = \"train\"\n",
    "\n",
    "# 이미지 및 Segmentation_mask를 224x224로 변환\n",
    "resize_transform = ResizeAndTransformBoth((224, 224))\n",
    "\n",
    "class CustomTransforms:\n",
    "    def __init__(self, resize_transform, tensor_transform, normalize_transform):\n",
    "        self.resize_transform = resize_transform\n",
    "        self.tensor_transform = tensor_transform\n",
    "        self.normalize_transform = normalize_transform\n",
    "\n",
    "    def __call__(self, img, target, segmentation_mask):\n",
    "        img, _, segmentation_mask = self.resize_transform(img, target, segmentation_mask)\n",
    "        img = self.tensor_transform(img)\n",
    "        img = self.normalize_transform(img)\n",
    "        segmentation_mask = self.tensor_transform(segmentation_mask)\n",
    "        return img, target, segmentation_mask\n",
    "    \n",
    "# 데이터셋을 정의하고 변환 작업 추가\n",
    "transforms = CustomTransforms(\n",
    "    resize_transform,\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    ")\n",
    "\n",
    "dataset = VOCSegmentation(root, year, image_set, download=False, transforms=transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋을 확인하는 부분 (예시)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_contours_on_image(image, segmentation_mask, color=(0, 255, 0), thickness=2):\n",
    "    # 이미지와 세그멘테이션 마스크를 NumPy 배열로 변환\n",
    "    image_np = np.array(image)\n",
    "    segmentation_mask_np = np.array(segmentation_mask)\n",
    "\n",
    "    # 세그멘테이션 마스크의 각 고유한 라벨에 대해 컨투어를 찾아 이미지에 그림\n",
    "    for label in np.unique(segmentation_mask_np):\n",
    "        if label == 0:  # 배경에 해당하는 라벨을 무시\n",
    "            continue\n",
    "\n",
    "        mask = np.zeros_like(segmentation_mask_np)\n",
    "        mask[segmentation_mask_np == label] = 255\n",
    "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # 컨투어를 이미지에 그림\n",
    "        cv2.drawContours(image_np, contours, -1, color, thickness)\n",
    "\n",
    "    return image_np\n",
    "\n",
    "# 데이터셋에서 샘플 이미지 및 세그멘테이션 마스크를 가져옴\n",
    "img, target, segmentation_mask = dataset[0]\n",
    "\n",
    "# 컨투어를 그릴 이미지를 PIL 이미지로 변환\n",
    "image_with_contours = Image.fromarray(draw_contours_on_image(img, segmentation_mask))\n",
    "\n",
    "# 이미지를 출력\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_with_contours)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17119ea72eb6b909bd341f4b0d7a48b5939aea29e9bd033254fedca863285074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
