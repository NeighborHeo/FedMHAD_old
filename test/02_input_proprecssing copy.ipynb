{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch \n",
    "import mymodels \n",
    "import mydataset \n",
    "from torch.utils.data import DataLoader\n",
    "from utils.myfed import *\n",
    "import yaml\n",
    "# %%\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yamlfilepath = pathlib.Path.cwd().parent.joinpath('config.yaml')\n",
    "args = yaml.load(yamlfilepath.open('r'), Loader=yaml.FullLoader)\n",
    "args = argparse.Namespace(**args)\n",
    "args.datapath = \"~/.data\"\n",
    "args.batchsize = 200\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "\n",
    "# 1. data\n",
    "args.datapath = os.path.expanduser(args.datapath)\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    publicdata = 'cifar100'\n",
    "    args.N_class = 10\n",
    "elif args.dataset == 'cifar100':\n",
    "    publicdata = 'imagenet'\n",
    "    args.N_class = 100\n",
    "elif args.dataset == 'pascal_voc2012':\n",
    "    publicdata = 'mscoco'\n",
    "    args.N_class = 20\n",
    "\n",
    "assert args.dataset in ['cifar10', 'cifar100', 'pascal_voc2012']\n",
    "\n",
    "priv_data, _, test_dataset, public_dataset, distill_loader = mydataset.data_cifar.dirichlet_datasplit(\n",
    "    args, privtype=args.dataset, publictype=publicdata, N_parties=args.N_parties, online=not args.oneshot, public_percent=args.public_percent)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=args.batchsize, shuffle=False, num_workers=args.num_workers, sampler=None)\n",
    "val_loader = DataLoader(\n",
    "    dataset=public_dataset, batch_size=args.batchsize, shuffle=False, num_workers=args.num_workers, sampler=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name = 'vit_tiny_patch16_224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class)\n",
    "net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "loadname = os.path.join(\"/home/suncheol/code/VFL/FedMAD/checkpoints_backup/pascal_voc2012/a1.0+sd1+e300+b16+lkl\", str(n)+'.pt')\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local{n}......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadname = os.path.join(\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b16+lkl/model-0.pth\")\n",
    "loadname = os.path.join(\"/home/suncheol/code/FedTest/FedMAD2/checkpoints/pascal_voc2012/a1.0+sd1+e300+b128+lkl+slmha/oneshot_c1_q0.0_n0.0/q0.0_n0.0_ADAM_b128_5e-05_200_5e-05_m0.9_e10_0.66.pt\")\n",
    "# loadname = os.path.join(\"/home/suncheol/code/FedTest/pytorch-models/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_-1/ckpt.pth\")\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "models = []\n",
    "for i in range(0, 5):\n",
    "    model = copy.deepcopy(net)\n",
    "    loadname = os.path.join(f\"/home/suncheol/code/FedTest/FedMAD2/checkpoints/pascal_voc2012/a1.0+sd1+e300+b128+lkl+slmha/model-{i}.pth\")\n",
    "    # loadname = os.path.join(f\"/home/suncheol/code/FedTest/pytorch-models/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_{i}/ckpt.pth\")\n",
    "    if os.path.exists(loadname):\n",
    "        localmodels = torch.load(loadname)\n",
    "        #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "        logging.info(f'Loading Local......', 'filepath : ', loadname)\n",
    "        utils.load_dict(loadname, model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 1 batch of data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels, _ = dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "# multi label to multi captions\n",
    "def multi_label_to_multi_captions(labels):\n",
    "    captions = []\n",
    "    for label in labels:\n",
    "        caption = []\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == 1:\n",
    "                caption.append(VOC_CLASSES[i])\n",
    "        captions.append(caption)\n",
    "    return captions\n",
    "labels = multi_label_to_multi_captions(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = []\n",
    "pred_labels = []\n",
    "for model in models:\n",
    "    grad_cam_images.append(model.module.get_class_activation_map(images, labels))\n",
    "    m = torch.nn.Sigmoid()\n",
    "    th = 0.3\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred = multi_label_to_multi_captions(outputs)\n",
    "    pred_labels.append(pred)\n",
    "# grayscale_cam = net.module.get_class_activation_map(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_grad_cam_images = []\n",
    "central_model = copy.deepcopy(net)\n",
    "central_grad_cam_images.append(central_model.module.get_class_activation_map(images, labels))\n",
    "m = torch.nn.Sigmoid()\n",
    "th = 0.3\n",
    "outputs = m(central_model(images)).detach().cpu().numpy()\n",
    "outputs[outputs > th] = 1\n",
    "outputs[outputs <= th] = 0\n",
    "central_pred_labels = multi_label_to_multi_captions(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = torch.stack([torch.tensor(grad_cam_images[i]) for i in range(len(grad_cam_images))])\n",
    "grad_cam_images.shape # n_clients * b * 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_cam = torch.max(grad_cam_images, dim=0)[0]\n",
    "intersection_cam = torch.min(grad_cam_images, dim=0)[0]\n",
    "union_cam.cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_border_color(true_label, pred_label):\n",
    "    if pred_label == true_label:\n",
    "        return 'lime'  # green\n",
    "    elif set(pred_label) & set(true_label):\n",
    "        return 'gold'  # yellow\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "row = 4\n",
    "col = 9\n",
    "clients = 5\n",
    "extra_plots = 3  # union, intersection, global\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "\n",
    "for j in range(row):\n",
    "    true_label = labels[j]\n",
    "    for i in range(clients + extra_plots + 1):\n",
    "        ax = plt.subplot(row, col, j * col + i + 1)\n",
    "        # print(i, j)\n",
    "        if i == 0:\n",
    "            img = images[j].cpu().permute(1, 2, 0)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'GT: {true_label}')\n",
    "        elif i <= clients:\n",
    "            img = grad_cam_images[i - 1].cpu()[j]\n",
    "            pred_label = pred_labels[i - 1][j]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            plt.imshow(img)\n",
    "            # set border color\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(5)\n",
    "            plt.title(f'client{i}: {pred_label}')\n",
    "        elif i == clients + 1:\n",
    "            img = union_cam.cpu()[j]\n",
    "            plt.imshow(img)\n",
    "            plt.title('union')\n",
    "        elif i == clients + 2:\n",
    "            img = intersection_cam.cpu()[j]\n",
    "            plt.imshow(img)\n",
    "            plt.title('intersection')\n",
    "        elif i == clients + 3:\n",
    "            img = central_grad_cam_images[0][j]\n",
    "            border_color = get_border_color(true_label, central_pred_labels[j])\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'global: {central_pred_labels[j]}')\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(5)\n",
    "        # plt.gca().set_xticks([])\n",
    "        # plt.gca().set_yticks([])\n",
    "        # plt.gca().spines['top'].set_visible(False)\n",
    "        # plt.gca().spines['right'].set_visible(False)\n",
    "        # plt.gca().spines['bottom'].set_visible(False)\n",
    "        # plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict = {}\n",
    "for idx, (true_label, central_pred) in enumerate(zip(labels, central_pred_labels)):\n",
    "    correct_clients_count = sum(client_pred[idx] == true_label for client_pred in pred_labels)\n",
    "    correct_central = central_pred == true_label\n",
    "    correct_dict[idx] = (correct_clients_count, correct_central)\n",
    "\n",
    "# correct_dict to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(correct_dict, orient='index', columns=['correct_clients_count', 'correct_central'])\n",
    "df = df.pivot_table(index='correct_clients_count', columns='correct_central', aggfunc=len, fill_value=0)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_clients_correct_central(true_labels, client_pred_labels, central_pred_labels):\n",
    "    result_indices = []\n",
    "    for idx, (true_label, central_pred) in enumerate(zip(true_labels, central_pred_labels)):\n",
    "        if sum(client_pred[idx] == true_label for client_pred in client_pred_labels) < 2 and central_pred == true_label:\n",
    "            result_indices.append(idx)\n",
    "    return result_indices\n",
    "\n",
    "def find_correct_clients_wrong_central(true_labels, client_pred_labels, central_pred_labels):\n",
    "    result_indices = []\n",
    "    for idx, (true_label, central_pred) in enumerate(zip(true_labels, central_pred_labels)):\n",
    "        # if all(client_pred[idx] == true_label for client_pred in client_pred_labels) and central_pred != true_label:\n",
    "        #     result_indices.append(idx)\n",
    "        # more than 3 clients are correct and central is wrong\n",
    "        if sum(client_pred[idx] == true_label for client_pred in client_pred_labels) > 3 and central_pred != true_label:\n",
    "            result_indices.append(idx)\n",
    "            \n",
    "    return result_indices\n",
    "\n",
    "# Example usage\n",
    "wrong_clients_correct_central_indices = find_wrong_clients_correct_central(labels, pred_labels, central_pred_labels[0])\n",
    "print(\"Wrong clients, correct central indices:\", wrong_clients_correct_central_indices)\n",
    "\n",
    "correct_clients_wrong_central_indices = find_correct_clients_wrong_central(labels, pred_labels, central_pred_labels[0])\n",
    "print(\"Correct clients, wrong central indices:\", correct_clients_wrong_central_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_border_color(true_label, pred_label):\n",
    "    if pred_label == true_label:\n",
    "        return 'lime'  # green\n",
    "    elif set(pred_label) & set(true_label):\n",
    "        return 'gold'  # yellow\n",
    "    else:\n",
    "        return 'red'\n",
    "image_index = [58, 110, 127]\n",
    "row = len(image_index)\n",
    "col = 9\n",
    "clients = 5\n",
    "extra_plots = 3  # union, intersection, global\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "\n",
    "for j in range(row):\n",
    "    idxImage = image_index[j]\n",
    "    true_label = labels[idxImage]\n",
    "    for i in range(clients + extra_plots + 1):\n",
    "        ax = plt.subplot(row, col, j * col + i + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # print(i, idxImage)\n",
    "        if i == 0:\n",
    "            img = images[idxImage].cpu().permute(1, 2, 0)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'GT: {true_label}')\n",
    "        elif i <= clients:\n",
    "            img = grad_cam_images[i - 1].cpu()[idxImage]\n",
    "            pred_label = pred_labels[i - 1][idxImage]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            plt.imshow(img)\n",
    "            # set border color\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "            plt.title(f'client{i}: {pred_label}')\n",
    "        elif i == clients + 1:\n",
    "            img = union_cam.cpu()[idxImage]\n",
    "            plt.imshow(img)\n",
    "            plt.title('union')\n",
    "        elif i == clients + 2:\n",
    "            img = intersection_cam.cpu()[idxImage]\n",
    "            plt.imshow(img)\n",
    "            plt.title('intersection')\n",
    "        elif i == clients + 3:\n",
    "            img = central_grad_cam_images[0][idxImage]\n",
    "            border_color = get_border_color(true_label, central_pred_labels[idxImage])\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'global: {central_pred_labels[idxImage]}')\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "        # plt.gca().set_xticks([])\n",
    "        # plt.gca().set_yticks([])\n",
    "        # plt.gca().spines['top'].set_visible(False)\n",
    "        # plt.gca().spines['right'].set_visible(False)\n",
    "        # plt.gca().spines['bottom'].set_visible(False)\n",
    "        # plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.savefig('test.png')\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_images = [] \n",
    "th_images = []\n",
    "for model in models:\n",
    "    mha, th = model.module.get_attention_maps_postprocessing_(images.cuda())\n",
    "    mha_images.append(mha)\n",
    "    th_images.append(th)\n",
    "    \n",
    "mha_images = torch.stack([torch.tensor(mha_images[i]) for i in range(len(mha_images))])\n",
    "th_images = torch.stack([torch.tensor(th_images[i]) for i in range(len(th_images))])\n",
    "mha_images.shape # n_clients * b * 224 * 224\n",
    "th_images.shape # n_clients * b * 224 * 224\n",
    "print(mha_images.shape, th_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_mha, central_th = central_model.module.get_attention_maps_postprocessing_(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mha_images(images, labels, mha_images, pred_labels, central_mha, central_pred_labels):\n",
    "    n_clients, n_images, n_head, h, w = mha_images.shape\n",
    "    image_indices = [58, 110, 127]\n",
    "    row = len(image_indices) * n_head\n",
    "    col = 1 + n_clients + 1\n",
    "    plt.figure(figsize=(3 * col, 3 * row))\n",
    "    for j in range(0, row):\n",
    "        _j = j // n_head\n",
    "        img_index = image_indices[_j]\n",
    "        true_label = labels[img_index]\n",
    "        k = j % n_head\n",
    "        if k == 0:\n",
    "            ax = plt.subplot(row, col, j * col + 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(images[img_index].numpy().transpose(1, 2, 0))\n",
    "            plt.title(true_label)\n",
    "        for i in range(0, n_clients):\n",
    "            ax = plt.subplot(row, col, j * col + i + 2)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(mha_images[i, img_index, k, :, :].numpy())\n",
    "            pred_label = pred_labels[i][img_index]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "            if k == 0:\n",
    "                plt.title(f'client {i}: {pred_label}')\n",
    "        ax = plt.subplot(row, col, j * col + n_clients + 2)\n",
    "        plt.imshow(central_mha[img_index, k, :, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        central_pred_label = central_pred_labels[img_index]\n",
    "        border_color = get_border_color(true_label, central_pred_label)\n",
    "        ax.patch.set_edgecolor(border_color)\n",
    "        ax.patch.set_linewidth(7)\n",
    "\n",
    "        if k == 0:\n",
    "            plt.title(f'global: {central_pred_label}')\n",
    "    plt.savefig('test2.png')\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Usage\n",
    "plot_mha_images(images, labels, mha_images, pred_labels, central_mha, central_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 파라미터 정의\n",
    "num_client = 5\n",
    "batchsize = 128\n",
    "num_head = 3\n",
    "imagesize_width = 224\n",
    "imagesize_height = 224\n",
    "\n",
    "# PCA를 수행하기 위해 데이터를 적절한 형태로 변환합니다.\n",
    "reshaped_mha_images = mha_images.reshape(num_client, batchsize, num_head, -1)\n",
    "\n",
    "# 각 헤드에 대해 PCA 수행\n",
    "for head_idx in range(num_head):\n",
    "    pca_input = reshaped_mha_images[:, :, head_idx, :].reshape(num_client * batchsize, -1).detach().numpy()\n",
    "\n",
    "    # PCA 수행\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_output = pca.fit_transform(pca_input)\n",
    "\n",
    "    # 결과 시각화\n",
    "    for client_idx in range(num_client):\n",
    "        start_idx = client_idx * batchsize\n",
    "        end_idx = (client_idx + 1) * batchsize\n",
    "        mean_coordinate = np.mean(pca_output[start_idx:end_idx, :], axis=0)\n",
    "        plt.scatter(mean_coordinate[0], mean_coordinate[1], label=f'Client {client_idx + 1}, Head {head_idx + 1}')\n",
    "\n",
    "# 시각화 옵션 설정 및 표시\n",
    "plt.legend()\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA of Multihead Attention Maps per Head')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 파라미터 정의\n",
    "num_client = 5\n",
    "batchsize = 128\n",
    "num_head = 3\n",
    "imagesize_width = 224\n",
    "imagesize_height = 224\n",
    "\n",
    "# PCA를 수행하기 위해 데이터를 적절한 형태로 변환합니다.\n",
    "reshaped_mha_images = mha_images.reshape(num_client, batchsize, num_head, -1)\n",
    "\n",
    "# 색상 팔레트 설정\n",
    "palette = sns.color_palette(\"husl\", num_head)\n",
    "\n",
    "# 각 헤드에 대해 PCA 수행\n",
    "for head_idx in range(num_head):\n",
    "    pca_input = reshaped_mha_images[:, :, head_idx, :].reshape(num_client * batchsize, -1).detach().numpy()\n",
    "\n",
    "    # PCA 수행\n",
    "    pca = PCA(n_components=5)\n",
    "    pca_output = pca.fit_transform(pca_input)\n",
    "\n",
    "    # 결과 시각화\n",
    "    for client_idx in range(num_client):\n",
    "        start_idx = client_idx * batchsize\n",
    "        end_idx = (client_idx + 1) * batchsize\n",
    "        # mean_coordinate = np.mean(pca_output[start_idx:end_idx, :], axis=0)\n",
    "        # plt.scatter(mean_coordinate[0], mean_coordinate[1], label=f'Client {client_idx + 1}, Head {head_idx + 1}', color=palette[head_idx])\n",
    "        plt.scatter(pca_output[start_idx:end_idx, 0], pca_output[start_idx:end_idx, 1], label=f'Client {client_idx + 1}, Head {head_idx + 1}', color=palette[head_idx])\n",
    "\n",
    "# 시각화 옵션 설정 및 표시\n",
    "# set legend position to outside of the plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Average PCA Coordinates of Multihead Attention Maps per Head and Client')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 파라미터 정의\n",
    "num_client = 5\n",
    "batchsize = 128\n",
    "num_head = 3\n",
    "imagesize_width = 224\n",
    "imagesize_height = 224\n",
    "\n",
    "# 클러스터 수 정의\n",
    "num_clusters = 3\n",
    "\n",
    "# PCA를 수행하기 위해 데이터를 적절한 형태로 변환합니다.\n",
    "reshaped_mha_images = mha_images.reshape(num_client, batchsize, num_head, -1)\n",
    "# batchsize = 1 \n",
    "# reshaped_mha_images = reshaped_mha_images[:, 0, :, :].reshape(num_client, batchsize, num_head, -1)\n",
    "print(reshaped_mha_images.shape)\n",
    "\n",
    "# 색상 팔레트 설정\n",
    "palette = sns.color_palette(\"husl\", num_clusters)\n",
    "\n",
    "# 각 헤드에 대해 t-SNE 및 K-means 클러스터링 수행\n",
    "\n",
    "for head_idx in range(num_head):\n",
    "    tsne_input = reshaped_mha_images[:, :, head_idx, :].reshape(num_client * batchsize, -1).detach().numpy()\n",
    "    print(tsne_input.shape)\n",
    "    \n",
    "    # t-SNE 수행\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    tsne_output = tsne.fit_transform(tsne_input.astype(np.float64))  # 데이터 형식을 double로 변환\n",
    "\n",
    "    # K-means 클러스터링 수행\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(tsne_output)\n",
    "\n",
    "    # 결과 시각화\n",
    "    for client_idx in range(num_client):\n",
    "        start_idx = client_idx * batchsize\n",
    "        end_idx = (client_idx + 1) * batchsize\n",
    "        coordinates = tsne_output[start_idx:end_idx, :]\n",
    "        for i, coord in enumerate(coordinates):\n",
    "            plt.scatter(coord[0], coord[1], label=f'Client {client_idx + 1}, Head {head_idx + 1}', color=palette[head_idx])\n",
    "\n",
    "# 시각화 옵션 설정 및 표시\n",
    "# plt.legend()\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('K-means Clustering of Average t-SNE Coordinates of Multihead Attention Maps per Head and Client')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline\n",
    "num_client = 5\n",
    "batchsize = 200\n",
    "num_head = 3\n",
    "imagesize_width = 224\n",
    "imagesize_height = 224\n",
    "\n",
    "batchsize = 200\n",
    "used_batchsize = 1\n",
    "\n",
    "temp_mha_images = mha_images[:, :used_batchsize, :, :]\n",
    "# 데이터 전처리\n",
    "head_images = []\n",
    "for client_idx in range(num_client):\n",
    "    for head_idx in range(num_head):\n",
    "        head_images.append(temp_mha_images[client_idx, :, head_idx].view(-1, imagesize_width * imagesize_height).numpy())\n",
    "\n",
    "head_images = np.vstack(head_images)\n",
    "\n",
    "\n",
    "# PCA 모델 생성 및 학습\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_images = pca.fit_transform(head_images)\n",
    "\n",
    "marker_list = [\"o\", \"s\", \"D\", \"<\", \">\"]\n",
    "palette = sns.color_palette(\"husl\", num_head)\n",
    "\n",
    "# t-SNE 결과 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "for client_idx in range(num_client):\n",
    "    for head_idx in range(num_head):\n",
    "        idx = client_idx * num_head + head_idx\n",
    "        start_idx = idx * used_batchsize\n",
    "        end_idx = (idx + 1) * used_batchsize\n",
    "        sns.scatterplot(\n",
    "            x=pca_images[start_idx:end_idx, 0],\n",
    "            y=pca_images[start_idx:end_idx, 1],\n",
    "            label=f\"Client {client_idx}, Head {head_idx}\",\n",
    "            color=palette[head_idx],\n",
    "            marker=marker_list[client_idx],\n",
    "            s=70\n",
    "        )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.title(\"PCA of Multihead Attention Maps per Head and Client\")\n",
    "\n",
    "# t-SNE 모델 생성 및 학습\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_images = tsne.fit_transform(head_images)\n",
    "\n",
    "# t-SNE 결과 시각화\n",
    "plt.subplot(2, 1, 2)\n",
    "for client_idx in range(num_client):\n",
    "    for head_idx in range(num_head):\n",
    "        idx = client_idx * num_head + head_idx\n",
    "        start_idx = idx * used_batchsize\n",
    "        end_idx = (idx + 1) * used_batchsize\n",
    "        sns.scatterplot(\n",
    "            x=tsne_images[start_idx:end_idx, 0],\n",
    "            y=tsne_images[start_idx:end_idx, 1],\n",
    "            label=f\"Client {client_idx}, Head {head_idx}\",\n",
    "            color=palette[head_idx],\n",
    "            marker=marker_list[client_idx],\n",
    "            s=70\n",
    "        )\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.title(\"t-SNE of Multihead Attention Maps per Head and Client\")\n",
    "plt.savefig(\"pca_tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arange = \n",
    "arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설명된 분산 계산\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# 설명된 분산과 누적 설명된 분산을 데이터프레임으로 변환\n",
    "variance_df = pd.DataFrame({'Component': np.arange(1, len(explained_variance) + 1),\n",
    "                            'Explained Variance': explained_variance,\n",
    "                            'Cumulative Explained Variance': cumulative_explained_variance})\n",
    "\n",
    "# 매트릭스 형태로 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x='Component', y='Explained Variance', data=variance_df, palette=\"Blues_d\", ci=None)\n",
    "sns.lineplot(x=np.arange(0, 2), y='Cumulative Explained Variance', data=variance_df, ax=ax, color='r', marker='o', ci=None)\n",
    "# no grid\n",
    "# ax2.grid(False)\n",
    "# 라벨 설정\n",
    "ax.set_xlabel('Principal Components')\n",
    "ax.set_ylabel('Explained Variance')\n",
    "# ax2.set_ylabel('Cumulative Explained Variance')\n",
    "# 범례 추가\n",
    "ax.legend(['Cumulative Explained Variance'], loc='upper left')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.savefig(\"explained_variance.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "565e3544d5dbeb515a1265a05ceb357b4338ebedb8b2db99297d61f63f17eeee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
