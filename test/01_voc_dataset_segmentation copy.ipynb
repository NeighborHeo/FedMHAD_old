{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suncheol/code/FedTest/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/suncheol/code/FedTest/.venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_unique_classes(dataset):\n",
    "    unique_classes = set()\n",
    "    for img, mask in dataset:\n",
    "        classes = list(mask.getcolors())\n",
    "        for count, pixel_value in classes:\n",
    "            unique_classes.add(pixel_value)\n",
    "    \n",
    "    return unique_classes\n",
    "\n",
    "# 데이터셋 다운로드 및 생성\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224), interpolation=Image.NEAREST), \n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "voc_dataset = VOCSegmentation(root='~/.data', year='2012', image_set='val', download=False, transform=transforms, target_transform=mask_transforms)\n",
    "voc_dataloader = torch.utils.data.DataLoader(voc_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_img_to_target(mask, num_classes=20):\n",
    "    # get colors\n",
    "    mask = to_pil_image(mask)\n",
    "    colors = list(mask.getcolors())\n",
    "    torch_target = torch.zeros(num_classes)\n",
    "    for count, pixel_value in colors:\n",
    "        if pixel_value in [0, 255]: # 0 is background, 255 is border,\n",
    "            continue\n",
    "        torch_target[pixel_value - 1] = 1\n",
    "    return torch_target\n",
    "\n",
    "def masks_to_targets(masks, num_classes=20):\n",
    "    targets = []\n",
    "    for mask in masks:\n",
    "        targets.append(mask_img_to_target(mask, num_classes))\n",
    "    return torch.stack(targets)\n",
    "\n",
    "images, masks = iter(voc_dataloader).next()\n",
    "labels = masks_to_targets(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_images(dataset, num_images=5):\n",
    "#     fig, ax = plt.subplots(num_images, 3, figsize=(5, num_images * 2))\n",
    "#     for i in range(num_images):\n",
    "#         # 이미지 및 마스크 가져오기\n",
    "#         img, mask = dataset[i]\n",
    "\n",
    "#         # 이미지 및 마스크 표시\n",
    "#         ax[i, 0].imshow(img.permute(1, 2, 0))\n",
    "#         ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "#         ax[i, 1].imshow(mask.permute(1, 2, 0), cmap='gray')\n",
    "#         ax[i, 1].set_title(f\"Segmentation Image {i + 1}\")\n",
    "#         # overlay segmentation mask on image\n",
    "#         ax[i, 2].imshow(img.permute(1, 2, 0))\n",
    "#         ax[i, 2].imshow(mask.permute(1, 2, 0), alpha=0.4, cmap='gray')\n",
    "#         ax[i, 2].set_title(f\"Overlay {i + 1}\")\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# display_images(voc_dataset)\n",
    "# def display_images(images, masks):\n",
    "#     num_images = len(images)\n",
    "#     print(num_images)\n",
    "#     fig, ax = plt.subplots(num_images, 3, figsize=(5, num_images * 2))\n",
    "#     for i in range(num_images):\n",
    "#         # 이미지 및 마스크 가져오기\n",
    "#         img = images[i].permute(1, 2, 0)\n",
    "#         mask = masks[i].permute(1, 2, 0)\n",
    "\n",
    "#         # 이미지 및 마스크 표시\n",
    "#         ax[i, 0].imshow(img)\n",
    "#         ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "#         ax[i, 1].imshow(mask, cmap='gray')\n",
    "#         ax[i, 1].set_title(f\"Segmentation Image {i + 1}\")\n",
    "#         # overlay segmentation mask on image\n",
    "#         ax[i, 2].imshow(img)\n",
    "#         ax[i, 2].imshow(mask, alpha=0.4, cmap='gray')\n",
    "#         ax[i, 2].set_title(f\"Overlay {i + 1}\")\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('test.png')\n",
    "#     plt.show()\n",
    "    \n",
    "# # 데이터셋에서 이미지 및 마스크 표시\n",
    "# display_images(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch \n",
    "import mymodels \n",
    "import mydataset \n",
    "from torch.utils.data import DataLoader\n",
    "from utils.myfed import *\n",
    "import yaml\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamlfilepath = pathlib.Path.cwd().parent.joinpath('config.yaml')\n",
    "args = yaml.load(yamlfilepath.open('r'), Loader=yaml.FullLoader)\n",
    "args = argparse.Namespace(**args)\n",
    "args.datapath = \"~/.data\"\n",
    "args.batchsize = 200\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "# 1. data\n",
    "args.datapath = os.path.expanduser(args.datapath)\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    publicdata = 'cifar100'\n",
    "    args.N_class = 10\n",
    "elif args.dataset == 'cifar100':\n",
    "    publicdata = 'imagenet'\n",
    "    args.N_class = 100\n",
    "elif args.dataset == 'pascal_voc2012':\n",
    "    publicdata = 'mscoco'\n",
    "    args.N_class = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=192, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name = 'vit_tiny_patch16_224'\n",
    "net = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class)\n",
    "net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model1 = copy.deepcopy(net)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h3/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6575.pt', model1)\n",
    "model2 = copy.deepcopy(net)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel/a1.0+sd1+e300+b64+lkl+slNone/oneshot_c1_q0.0_n0.0_h0/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6563.pt', model2)\n",
    "model1 = copy.deepcopy(net)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_only/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h2/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e10_0.5234.pt', model1)\n",
    "model2 = copy.deepcopy(net)\n",
    "# utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_singlelabel/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e20_0.76.pt', model2)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_only/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h1/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e10_0.5270.pt', model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dicescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['boat'],\n",
       " ['bird', 'bus'],\n",
       " ['chair', 'diningtable', 'pottedplant'],\n",
       " ['chair', 'person', 'tvmonitor'],\n",
       " ['bottle', 'person', 'tvmonitor'],\n",
       " ['train'],\n",
       " ['cat', 'chair'],\n",
       " ['bottle', 'diningtable', 'person'],\n",
       " ['bird'],\n",
       " ['sheep'],\n",
       " ['car'],\n",
       " ['sheep'],\n",
       " ['bus'],\n",
       " ['motorbike', 'person'],\n",
       " ['boat'],\n",
       " ['pottedplant', 'sofa'],\n",
       " ['dog', 'person'],\n",
       " ['horse'],\n",
       " ['bus', 'car', 'person'],\n",
       " ['diningtable', 'person'],\n",
       " ['car'],\n",
       " ['bicycle', 'person'],\n",
       " ['bicycle', 'pottedplant'],\n",
       " ['pottedplant'],\n",
       " ['aeroplane'],\n",
       " ['pottedplant', 'tvmonitor'],\n",
       " ['bus'],\n",
       " ['tvmonitor'],\n",
       " ['cat'],\n",
       " ['dog'],\n",
       " ['chair', 'sofa'],\n",
       " ['sheep'],\n",
       " ['horse'],\n",
       " ['chair', 'diningtable'],\n",
       " ['motorbike', 'person'],\n",
       " ['train'],\n",
       " ['tvmonitor'],\n",
       " ['bicycle', 'car', 'person'],\n",
       " ['bus', 'car', 'person'],\n",
       " ['sofa'],\n",
       " ['bus'],\n",
       " ['chair', 'person'],\n",
       " ['diningtable', 'person'],\n",
       " ['bicycle'],\n",
       " ['bus'],\n",
       " ['car', 'motorbike', 'person'],\n",
       " ['aeroplane'],\n",
       " ['cat', 'dog', 'sofa', 'tvmonitor'],\n",
       " ['chair'],\n",
       " ['cat'],\n",
       " ['aeroplane'],\n",
       " ['horse'],\n",
       " ['bird'],\n",
       " ['cow'],\n",
       " ['person'],\n",
       " ['dog'],\n",
       " ['person', 'sofa'],\n",
       " ['cow'],\n",
       " ['bird'],\n",
       " ['cat'],\n",
       " ['bus', 'car'],\n",
       " ['cow'],\n",
       " ['cat', 'person'],\n",
       " ['bottle'],\n",
       " ['bicycle'],\n",
       " ['train'],\n",
       " ['aeroplane'],\n",
       " ['bottle'],\n",
       " ['tvmonitor'],\n",
       " ['cat', 'chair', 'diningtable'],\n",
       " ['car', 'motorbike'],\n",
       " ['cat'],\n",
       " ['bicycle', 'person'],\n",
       " ['person'],\n",
       " ['person'],\n",
       " ['bus'],\n",
       " ['sofa'],\n",
       " ['bottle'],\n",
       " ['dog'],\n",
       " ['bus', 'car', 'person'],\n",
       " ['aeroplane', 'person'],\n",
       " ['boat'],\n",
       " ['bird'],\n",
       " ['aeroplane'],\n",
       " ['tvmonitor'],\n",
       " ['horse'],\n",
       " ['bird'],\n",
       " ['car', 'train'],\n",
       " ['cat', 'person'],\n",
       " ['bottle'],\n",
       " ['motorbike', 'person'],\n",
       " ['boat'],\n",
       " ['dog', 'sofa'],\n",
       " ['bus', 'person'],\n",
       " ['cat'],\n",
       " ['cow', 'person'],\n",
       " ['bird'],\n",
       " ['aeroplane'],\n",
       " ['dog'],\n",
       " ['dog'],\n",
       " ['bicycle', 'bottle', 'person'],\n",
       " ['sheep'],\n",
       " ['bottle', 'person'],\n",
       " ['dog'],\n",
       " ['tvmonitor'],\n",
       " ['car', 'person'],\n",
       " ['cow'],\n",
       " ['boat'],\n",
       " ['cow'],\n",
       " ['car'],\n",
       " ['cat'],\n",
       " ['chair', 'person', 'pottedplant'],\n",
       " ['bottle'],\n",
       " ['sheep'],\n",
       " ['person'],\n",
       " ['aeroplane'],\n",
       " ['boat'],\n",
       " ['train'],\n",
       " ['aeroplane'],\n",
       " ['chair', 'person', 'tvmonitor'],\n",
       " ['bird'],\n",
       " ['chair', 'person'],\n",
       " ['diningtable', 'person'],\n",
       " ['motorbike', 'person'],\n",
       " ['cat'],\n",
       " ['bus', 'car', 'person'],\n",
       " ['bird'],\n",
       " ['boat'],\n",
       " ['dog', 'person'],\n",
       " ['dog'],\n",
       " ['pottedplant', 'tvmonitor'],\n",
       " ['cow'],\n",
       " ['bird', 'person', 'pottedplant'],\n",
       " ['aeroplane', 'person'],\n",
       " ['boat'],\n",
       " ['bird'],\n",
       " ['horse'],\n",
       " ['horse', 'person'],\n",
       " ['bicycle'],\n",
       " ['chair', 'sofa'],\n",
       " ['dog'],\n",
       " ['bird'],\n",
       " ['motorbike'],\n",
       " ['bird'],\n",
       " ['boat'],\n",
       " ['chair', 'diningtable', 'person'],\n",
       " ['aeroplane'],\n",
       " ['chair', 'diningtable'],\n",
       " ['horse'],\n",
       " ['dog'],\n",
       " ['dog'],\n",
       " ['bird', 'pottedplant'],\n",
       " ['cat'],\n",
       " ['aeroplane'],\n",
       " ['chair', 'diningtable'],\n",
       " ['cat'],\n",
       " ['bottle', 'person'],\n",
       " ['car'],\n",
       " ['cow', 'person'],\n",
       " ['cat'],\n",
       " ['bus'],\n",
       " ['car', 'motorbike'],\n",
       " ['train'],\n",
       " ['car', 'person'],\n",
       " ['train'],\n",
       " ['dog'],\n",
       " ['dog'],\n",
       " ['sheep'],\n",
       " ['aeroplane'],\n",
       " ['horse'],\n",
       " ['cow'],\n",
       " ['sofa'],\n",
       " ['diningtable', 'person'],\n",
       " ['cat'],\n",
       " ['aeroplane', 'person'],\n",
       " ['bottle', 'person'],\n",
       " ['cow'],\n",
       " ['bus'],\n",
       " ['bus'],\n",
       " ['horse', 'person'],\n",
       " ['horse'],\n",
       " ['cat', 'dog'],\n",
       " ['cat'],\n",
       " ['bottle', 'tvmonitor'],\n",
       " ['person'],\n",
       " ['bird', 'person'],\n",
       " ['chair', 'diningtable'],\n",
       " ['car'],\n",
       " ['person'],\n",
       " ['aeroplane'],\n",
       " ['bicycle', 'person'],\n",
       " ['horse'],\n",
       " ['horse'],\n",
       " ['person', 'train'],\n",
       " ['bottle', 'person'],\n",
       " ['bird'],\n",
       " ['horse'],\n",
       " ['pottedplant'],\n",
       " ['chair', 'person'],\n",
       " ['car', 'pottedplant']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOC_CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "# multi label to multi captions\n",
    "def multi_label_to_multi_captions(labels):\n",
    "    # if already convert to captions\n",
    "    if isinstance(labels[0][0], str):\n",
    "        return labels\n",
    "    \n",
    "    captions = []\n",
    "    for label in labels:\n",
    "        caption = []\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == 1:\n",
    "                caption.append(VOC_CLASSES[i])\n",
    "        captions.append(caption)\n",
    "    return captions\n",
    "\n",
    "labels = multi_label_to_multi_captions(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = []\n",
    "pred_labels = []\n",
    "for model in models:\n",
    "    grad_cam_images.append(model.module.get_class_activation_map(images, labels))\n",
    "    m = torch.nn.Sigmoid()\n",
    "    th = 0.3\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred = multi_label_to_multi_captions(outputs)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "grad_cam_images = torch.stack([torch.tensor(grad_cam_images[i]) for i in range(len(grad_cam_images))])\n",
    "grad_cam_images.shape # n_clients * b * 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "def getAccuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "def get_Hamming_Loss(y_true, y_pred):\n",
    "    temp=0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += np.size(y_true[i] == y_pred[i]) - np.count_nonzero(y_true[i] == y_pred[i])\n",
    "    return temp/(y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "def getPrecision(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_true[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def getRecall(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_pred[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def getF1score(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
    "            continue\n",
    "        temp+= (2*sum(np.logical_and(y_true[i], y_pred[i])))/ (sum(y_true[i])+sum(y_pred[i]))\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def getMetrics(y_true, y_score, th):\n",
    "    y_pred = (y_score > th).astype(int)\n",
    "    acc = getAccuracy(y_true, y_pred)\n",
    "    pre = getPrecision(y_true, y_pred)\n",
    "    rec = getRecall(y_true, y_pred)\n",
    "    f1 = getF1score(y_true, y_pred)\n",
    "    return acc, pre, rec, f1\n",
    "\n",
    "def accuracyforsinglelabel(output, target, topk=(1,)):\n",
    "    output = torch.tensor(output)\n",
    "    target = torch.tensor(target)\n",
    "    if len(output.shape) == 2:\n",
    "        predicted = output.argmax(dim=1)\n",
    "    if len(target.shape) == 2:\n",
    "        target = target.argmax(dim=1)\n",
    "    \n",
    "    total, correct = 0, 0\n",
    "    total += target.size(0)\n",
    "    correct += predicted.eq(target).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    usage:\n",
    "    prec1,prec5=accuracy(output,target,topk=(1,5))\n",
    "    \"\"\"\n",
    "    # print(output.shape, target.shape)\n",
    "    \n",
    "    th_ls = [0.1 * i for i in range(10)]\n",
    "    opt_th = 0\n",
    "    best_acc = 0\n",
    "    for th in th_ls:\n",
    "        acc, pre, rec, f1 = getMetrics(target, output, th)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            opt_th = th\n",
    "\n",
    "    acc, pre, rec, f1 = getMetrics(target, output, opt_th)\n",
    "    print(f\"opt_th: {opt_th:.2f}, best_acc: {best_acc:.2f}, pre: {pre:.2f}, rec: {rec:.2f}, f1: {f1:.2f}\")\n",
    "        \n",
    "    res = []\n",
    "    for k in topk:\n",
    "        res.append(acc)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = model1.module.get_class_activation_map(images, labels) \n",
    "grad_cam_image2 = model2.module.get_class_activation_map(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_label(model, images, th=0.3):\n",
    "    m = torch.nn.Sigmoid()\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred = multi_label_to_multi_captions(outputs)\n",
    "    return pred\n",
    "pred_label1 = get_pred_label(model1, images, th=0.3)\n",
    "pred_label2 = get_pred_label(model2, images, th=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images), len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_score1</th>\n",
       "      <th>test_score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_score1  test_score2\n",
       "False          108          108\n",
       "True            92           92"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label1 == pred_label2\n",
    "test_score1 = [label == pred for label, pred in zip(labels, pred_label1)]\n",
    "test_score2 = [label == pred for label, pred in zip(labels, pred_label2)]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'test_score1': test_score1, 'test_score2': test_score2})\n",
    "df.apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict = {}\n",
    "for idx, (true_label, pred_label) in enumerate(zip(labels, pred_label1)):\n",
    "    correct_clients_count = client_pred[idx] == true_label for client_pred in pred_labels)\n",
    "    correct_central = pred_label == true_label\n",
    "    correct_dict[idx] = (correct_clients_count, correct_central)\n",
    "\n",
    "# correct_dict to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(correct_dict, orient='index', columns=['correct_clients_count', 'correct_central'])\n",
    "df = df.pivot_table(index='correct_clients_count', columns='correct_central', aggfunc=len, fill_value=0)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, median of grad_cam_image:  tensor(0.3056) tensor(0.3184)\n",
      "mean, median of grad_cam_image:  tensor(0.2437) tensor(0.2030)\n",
      "mean, median of grad_cam_image:  tensor(0.2055) tensor(0.1121)\n",
      "mean, median of grad_cam_image:  tensor(0.1670) tensor(0.1316)\n",
      "mean, median of grad_cam_image:  tensor(0.2151) tensor(0.1222)\n",
      "mean, median of grad_cam_image:  tensor(0.2624) tensor(0.2734)\n",
      "mean, median of grad_cam_image:  tensor(0.3220) tensor(0.2996)\n",
      "mean, median of grad_cam_image:  tensor(0.2489) tensor(0.1058)\n",
      "mean, median of grad_cam_image:  tensor(0.2290) tensor(0.2169)\n",
      "mean, median of grad_cam_image:  tensor(0.3118) tensor(0.3337)\n",
      "[tensor(0.1493), tensor(0.7580), tensor(0.7383), tensor(0.5802), tensor(0.0545), tensor(0.0784), tensor(0.3480), tensor(0.7645), tensor(0.0073), tensor(0.4655)]\n",
      "mean, median of grad_cam_image:  tensor(0.3234) tensor(0.2822)\n",
      "mean, median of grad_cam_image:  tensor(0.3267) tensor(0.3311)\n",
      "mean, median of grad_cam_image:  tensor(0.1800) tensor(0.0424)\n",
      "mean, median of grad_cam_image:  tensor(0.1514) tensor(0.0382)\n",
      "mean, median of grad_cam_image:  tensor(0.1919) tensor(0.0468)\n",
      "mean, median of grad_cam_image:  tensor(0.1728) tensor(0.0151)\n",
      "mean, median of grad_cam_image:  tensor(0.3094) tensor(0.2543)\n",
      "mean, median of grad_cam_image:  tensor(0.2821) tensor(0.2851)\n",
      "mean, median of grad_cam_image:  tensor(0.2532) tensor(0.2663)\n",
      "mean, median of grad_cam_image:  tensor(0.3426) tensor(0.2544)\n",
      "[tensor(0.2195), tensor(0.8743), tensor(0.7518), tensor(0.6408), tensor(0.7198), tensor(0.4375), tensor(0.3041), tensor(0.6012), tensor(0.1551), tensor(0.8997)]\n",
      "mean of dice score:  tensor(0.3944) tensor(0.5604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5219/506602218.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  central_grad_cam = torch.tensor(grad_cam_image[i] > torch.mean(grad_cam_image[i])).float()\n",
      "/tmp/ipykernel_5219/506602218.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  threshold_images.append(torch.tensor(grad_cam_image[i] > torch.median(grad_cam_image[i])).float())\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "def dice_score(y_pred, y_true, smooth=1):\n",
    "    if torch.max(y_pred) > 1:\n",
    "        print(\"y_pred should be in range [0, 1]\")\n",
    "    if torch.max(y_true) > 1:\n",
    "        print(\"y_true should be in range [0, 1]\")\n",
    "    y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "    dice_loss = (2 * (y_pred * y_true).sum() + smooth) / ((y_pred + y_true).sum() + smooth)\n",
    "    return dice_loss\n",
    "\n",
    "def calculate_dice_score(grad_cam_image, masks):\n",
    "    if not isinstance(grad_cam_image, torch.Tensor):\n",
    "        grad_cam_image = torch.tensor(grad_cam_image)\n",
    "        \n",
    "    dice_scores = []\n",
    "    for i in range(10):\n",
    "        print(\"mean, median of grad_cam_image: \", torch.mean(grad_cam_image[i]), torch.median(grad_cam_image[i])) \n",
    "        central_grad_cam = torch.tensor(grad_cam_image[i] > torch.mean(grad_cam_image[i])).float()\n",
    "        mask_img = masks[i].unsqueeze(0).cpu() > 0\n",
    "        ds = dice_score(central_grad_cam, mask_img)\n",
    "        dice_scores.append(ds)\n",
    "    print(dice_scores)\n",
    "    return dice_scores\n",
    "\n",
    "dice_scores = calculate_dice_score(grad_cam_image, masks)\n",
    "dice_scores2 = calculate_dice_score(grad_cam_image2, masks)\n",
    "print(\"mean of dice score: \", torch.mean(torch.tensor(dice_scores)), torch.mean(torch.tensor(dice_scores2)))\n",
    "\n",
    "def getThresholdImages(grad_cam_image):\n",
    "    if not isinstance(grad_cam_image, torch.Tensor):\n",
    "        grad_cam_image = torch.tensor(grad_cam_image)\n",
    "        \n",
    "    threshold_images = []\n",
    "    for i in range(len(grad_cam_image)):\n",
    "        threshold_images.append(torch.tensor(grad_cam_image[i] > torch.median(grad_cam_image[i])).float())\n",
    "    return threshold_images\n",
    "threshold_images = getThresholdImages(grad_cam_image)\n",
    "threshold_images2 = getThresholdImages(grad_cam_image2)\n",
    "\n",
    "def drawplots(images, masks, grad_cam_images, threshold_images):\n",
    "    length = len(images)\n",
    "    fig, ax = plt.subplots(length, 4, figsize=(20, 20))\n",
    "    for i in range(length):\n",
    "        ax[i, 0].imshow(images[i].permute(1, 2, 0))\n",
    "        ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "        ax[i, 1].imshow(masks[i].permute(1, 2, 0), alpha=0.4, cmap='gray')\n",
    "        ax[i, 1].set_title(f\"Mask {i + 1}\")\n",
    "        ax[i, 2].imshow(grad_cam_images[i])\n",
    "        ax[i, 2].set_title(f\"Grad CAM {i + 1}\")\n",
    "        ax[i, 3].imshow(threshold_images[i], alpha=0.4, cmap='gray')\n",
    "        ax[i, 3].set_title(f\"Threshold {i + 1}\")\n",
    "    plt.show()\n",
    "# drawplots(images, masks, grad_cam_image, threshold_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 224, 224])\n",
      "torch.Size([200, 224, 224])\n",
      "mean, median of grad_cam_image:  tensor(0.0034) tensor(0.0019)\n",
      "mean, median of grad_cam_image:  tensor(0.0051) tensor(0.0047)\n",
      "mean, median of grad_cam_image:  tensor(0.0046) tensor(0.0028)\n",
      "mean, median of grad_cam_image:  tensor(0.0059) tensor(0.0030)\n",
      "mean, median of grad_cam_image:  tensor(0.0056) tensor(0.0037)\n",
      "mean, median of grad_cam_image:  tensor(0.0051) tensor(0.0023)\n",
      "mean, median of grad_cam_image:  tensor(0.0065) tensor(0.0047)\n",
      "mean, median of grad_cam_image:  tensor(0.0036) tensor(0.0021)\n",
      "mean, median of grad_cam_image:  tensor(0.0049) tensor(0.0017)\n",
      "mean, median of grad_cam_image:  tensor(0.0053) tensor(0.0027)\n",
      "[tensor(0.4080), tensor(0.8271), tensor(0.6360), tensor(0.5680), tensor(0.5881), tensor(0.5388), tensor(0.5395), tensor(0.5606), tensor(0.4489), tensor(0.8326)]\n",
      "mean, median of grad_cam_image:  tensor(0.0048) tensor(0.0018)\n",
      "mean, median of grad_cam_image:  tensor(0.0063) tensor(0.0051)\n",
      "mean, median of grad_cam_image:  tensor(0.0060) tensor(0.0033)\n",
      "mean, median of grad_cam_image:  tensor(0.0067) tensor(0.0031)\n",
      "mean, median of grad_cam_image:  tensor(0.0070) tensor(0.0040)\n",
      "mean, median of grad_cam_image:  tensor(0.0054) tensor(0.0014)\n",
      "mean, median of grad_cam_image:  tensor(0.0071) tensor(0.0050)\n",
      "mean, median of grad_cam_image:  tensor(0.0057) tensor(0.0024)\n",
      "mean, median of grad_cam_image:  tensor(0.0057) tensor(0.0018)\n",
      "mean, median of grad_cam_image:  tensor(0.0061) tensor(0.0034)\n",
      "[tensor(0.4644), tensor(0.7142), tensor(0.4348), tensor(0.5054), tensor(0.3735), tensor(0.5716), tensor(0.5857), tensor(0.4114), tensor(0.5440), tensor(0.7507)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5219/506602218.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  central_grad_cam = torch.tensor(grad_cam_image[i] > torch.mean(grad_cam_image[i])).float()\n",
      "/tmp/ipykernel_5219/506602218.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  threshold_images.append(torch.tensor(grad_cam_image[i] > torch.median(grad_cam_image[i])).float())\n"
     ]
    }
   ],
   "source": [
    "mha, th = model1.module.get_attention_maps_postprocessing_(images.cuda())\n",
    "if not isinstance(mha, torch.Tensor):\n",
    "    mha = torch.tensor(mha)\n",
    "mha_agg = torch.max(mha, dim=1)[0]\n",
    "print(mha_agg.shape)\n",
    "mha2, th2 = model2.module.get_attention_maps_postprocessing_(images.cuda())\n",
    "if not isinstance(mha2, torch.Tensor):\n",
    "    mha2 = torch.tensor(mha2)\n",
    "mha_agg2 = torch.max(mha2, dim=1)[0]\n",
    "print(mha_agg2.shape)\n",
    "\n",
    "\n",
    "mha_dice_scores = calculate_dice_score(mha_agg, masks)\n",
    "mha_dice_scores2 = calculate_dice_score(mha_agg2, masks)\n",
    "\n",
    "mha_threshold_images = getThresholdImages(mha_agg)\n",
    "mha_threshold_images2 = getThresholdImages(mha_agg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mha_dice_scores: tensor(0.5948) tensor(0.5356)\n"
     ]
    }
   ],
   "source": [
    "print('mha_dice_scores:', torch.mean(torch.tensor(mha_dice_scores)), torch.mean(torch.tensor(mha_dice_scores2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawplots(images, masks, mha_agg, mha_threshold_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "loadname = os.path.join(\"/home/suncheol/code/VFL/FedMAD/checkpoints_backup/pascal_voc2012/a1.0+sd1+e300+b16+lkl\", str(n)+'.pt')\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local{n}......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadname = os.path.join(\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b16+lkl/model-0.pth\")\n",
    "loadname = os.path.join(\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b128+lkl+slmha/oneshot_c1_q0.0_n0.0/q0.0_n0.0_ADAM_b128_5e-05_200_5e-05_m0.9_e10_0.66.pt\")\n",
    "# loadname = os.path.join(\"/home/suncheol/code/FedTest/pytorch-models/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_-1/ckpt.pth\")\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "models = []\n",
    "for i in range(0, 5):\n",
    "    model = copy.deepcopy(net)\n",
    "    loadname = os.path.join(f\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b128+lkl+slmha/model-{i}.pth\")\n",
    "    # loadname = os.path.join(f\"/home/suncheol/code/FedTest/pytorch-models/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_{i}/ckpt.pth\")\n",
    "    if os.path.exists(loadname):\n",
    "        localmodels = torch.load(loadname)\n",
    "        #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "        logging.info(f'Loading Local......', 'filepath : ', loadname)\n",
    "        utils.load_dict(loadname, model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "# multi label to multi captions\n",
    "def multi_label_to_multi_captions(labels):\n",
    "    captions = []\n",
    "    for label in labels:\n",
    "        caption = []\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == 1:\n",
    "                caption.append(VOC_CLASSES[i])\n",
    "        captions.append(caption)\n",
    "    return captions\n",
    "labels = multi_label_to_multi_captions(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = []\n",
    "pred_labels = []\n",
    "for model in models:\n",
    "    grad_cam_images.append(model.module.get_class_activation_map(images, labels))\n",
    "    m = torch.nn.Sigmoid()\n",
    "    th = 0.3\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred = multi_label_to_multi_captions(outputs)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "grad_cam_images = torch.stack([torch.tensor(grad_cam_images[i]) for i in range(len(grad_cam_images))])\n",
    "grad_cam_images.shape # n_clients * b * 224 * 224\n",
    "\n",
    "# grayscale_cam = net.module.get_class_activation_map(images, labels)\n",
    "central_model = copy.deepcopy(net)\n",
    "central_grad_cam_image = central_model.module.get_class_activation_map(images, labels)\n",
    "m = torch.nn.Sigmoid()\n",
    "th = 0.3\n",
    "outputs = m(central_model(images)).detach().cpu().numpy()\n",
    "outputs[outputs > th] = 1\n",
    "outputs[outputs <= th] = 0\n",
    "central_pred_labels = multi_label_to_multi_captions(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_cam = torch.max(grad_cam_images, dim=0)[0]\n",
    "intersection_cam = torch.min(grad_cam_images, dim=0)[0]\n",
    "union_cam.cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_border_color(true_label, pred_label):\n",
    "    if pred_label == true_label:\n",
    "        return 'lime'  # green\n",
    "    elif set(pred_label) & set(true_label):\n",
    "        return 'gold'  # yellow\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "row = 4\n",
    "col = 9\n",
    "clients = 5\n",
    "extra_plots = 3  # union, intersection, global\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "\n",
    "for j in range(row):\n",
    "    true_label = labels[j]\n",
    "    for i in range(clients + extra_plots + 1):\n",
    "        ax = plt.subplot(row, col, j * col + i + 1)\n",
    "        # print(i, j)\n",
    "        if i == 0:\n",
    "            img = images[j].cpu().permute(1, 2, 0)\n",
    "            mask = masks[j].permute(1, 2, 0).cpu()\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'GT: {true_label}')\n",
    "        elif i <= clients:\n",
    "            img = grad_cam_images[i - 1].cpu()[j]\n",
    "            pred_label = pred_labels[i - 1][j]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            # set border color\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(5)\n",
    "            plt.title(f'client{i}: {pred_label}')\n",
    "        elif i == clients + 1:\n",
    "            img = union_cam.cpu()[j]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('union')\n",
    "        elif i == clients + 2:\n",
    "            img = intersection_cam.cpu()[j]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('intersection')\n",
    "        elif i == clients + 3:\n",
    "            img = central_grad_cam_image[j]\n",
    "            border_color = get_border_color(true_label, central_pred_labels[j])\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'global: {central_pred_labels[j]}')\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(5)\n",
    "        # plt.gca().set_xticks([])\n",
    "        # plt.gca().set_yticks([])\n",
    "        # plt.gca().spines['top'].set_visible(False)\n",
    "        # plt.gca().spines['right'].set_visible(False)\n",
    "        # plt.gca().spines['bottom'].set_visible(False)\n",
    "        # plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.savefig('grad_cam.png')\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict = {}\n",
    "for idx, (true_label, central_pred) in enumerate(zip(labels, central_pred_labels)):\n",
    "    correct_clients_count = sum(client_pred[idx] == true_label for client_pred in pred_labels)\n",
    "    correct_central = central_pred == true_label\n",
    "    correct_dict[idx] = (correct_clients_count, correct_central)\n",
    "\n",
    "# correct_dict to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(correct_dict, orient='index', columns=['correct_clients_count', 'correct_central'])\n",
    "df = df.pivot_table(index='correct_clients_count', columns='correct_central', aggfunc=len, fill_value=0)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_clients_correct_central(true_labels, client_pred_labels, central_pred_labels):\n",
    "    result_indices = []\n",
    "    for idx, (true_label, central_pred) in enumerate(zip(true_labels, central_pred_labels)):\n",
    "        if sum(client_pred[idx] == true_label for client_pred in client_pred_labels) < 2 and central_pred == true_label:\n",
    "            result_indices.append(idx)\n",
    "    return result_indices\n",
    "\n",
    "def find_correct_clients_wrong_central(true_labels, client_pred_labels, central_pred_labels):\n",
    "    result_indices = []\n",
    "    for idx, (true_label, central_pred) in enumerate(zip(true_labels, central_pred_labels)):\n",
    "        # if all(client_pred[idx] == true_label for client_pred in client_pred_labels) and central_pred != true_label:\n",
    "        #     result_indices.append(idx)\n",
    "        # more than 3 clients are correct and central is wrong\n",
    "        if sum(client_pred[idx] == true_label for client_pred in client_pred_labels) > 3 and central_pred != true_label:\n",
    "            result_indices.append(idx)\n",
    "            \n",
    "    return result_indices\n",
    "\n",
    "# Example usage\n",
    "wrong_clients_correct_central_indices = find_wrong_clients_correct_central(labels, pred_labels, central_pred_labels[0])\n",
    "print(\"Wrong clients, correct central indices:\", wrong_clients_correct_central_indices)\n",
    "\n",
    "correct_clients_wrong_central_indices = find_correct_clients_wrong_central(labels, pred_labels, central_pred_labels[0])\n",
    "print(\"Correct clients, wrong central indices:\", correct_clients_wrong_central_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(y_pred, y_true, smooth=1):\n",
    "    if torch.max(y_pred) > 1:\n",
    "        print(\"y_pred should be in range [0, 1]\")\n",
    "    if torch.max(y_true) > 1:\n",
    "        print(\"y_true should be in range [0, 1]\")\n",
    "    y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "    dice_loss = (2 * (y_pred * y_true).sum() + smooth) / ((y_pred + y_true).sum() + smooth)\n",
    "    return dice_loss\n",
    "\n",
    "dice_scores = []\n",
    "for i in range(10):\n",
    "    central_grad_cam = torch.tensor(central_grad_cam_image[i] >0.1).unsqueeze(0).cpu()\n",
    "    mask_img = masks[i].unsqueeze(0).cpu() > 0\n",
    "    ds = dice_score(central_grad_cam, mask_img)\n",
    "    dice_scores.append(ds)\n",
    "\n",
    "print(dice_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_border_color(true_label, pred_label):\n",
    "    if pred_label == true_label:\n",
    "        return 'lime'  # green\n",
    "    elif set(pred_label) & set(true_label):\n",
    "        return 'gold'  # yellow\n",
    "    else:\n",
    "        return 'red'\n",
    "image_index = [1, 4, 5, 8]\n",
    "row = len(image_index)\n",
    "col = 9\n",
    "clients = 5\n",
    "extra_plots = 3  # union, intersection, global\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "\n",
    "for j in range(row):\n",
    "    idxImage = image_index[j]\n",
    "    true_label = labels[idxImage]\n",
    "    for i in range(clients + extra_plots + 1):\n",
    "        ax = plt.subplot(row, col, j * col + i + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # print(i, idxImage)\n",
    "        if i == 0:\n",
    "            img = images[idxImage].cpu().permute(1, 2, 0)\n",
    "            mask = masks[idxImage].cpu().permute(1, 2, 0)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'GT: {true_label}')\n",
    "        elif i <= clients:\n",
    "            img = grad_cam_images[i - 1].cpu()[idxImage]\n",
    "            pred_label = pred_labels[i - 1][idxImage]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            # set border color\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "            plt.title(f'client{i}: {pred_label}')\n",
    "        elif i == clients + 1:\n",
    "            img = union_cam.cpu()[idxImage]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('union')\n",
    "        elif i == clients + 2:\n",
    "            img = intersection_cam.cpu()[idxImage]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('intersection')\n",
    "        elif i == clients + 3:\n",
    "            img = central_grad_cam_image[idxImage]\n",
    "            border_color = get_border_color(true_label, central_pred_labels[idxImage])\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'global: {central_pred_labels[idxImage]}, dice: {dice_scores[idxImage]:.2f}')\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "        # plt.gca().set_xticks([])\n",
    "        # plt.gca().set_yticks([])\n",
    "        # plt.gca().spines['top'].set_visible(False)\n",
    "        # plt.gca().spines['right'].set_visible(False)\n",
    "        # plt.gca().spines['bottom'].set_visible(False)\n",
    "        # plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.savefig('grad_cam2.png')\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_images = [] \n",
    "th_images = []\n",
    "for model in models:\n",
    "    mha, th = model.module.get_attention_maps_postprocessing_(images.cuda())\n",
    "    mha_images.append(mha)\n",
    "    th_images.append(th)\n",
    "    \n",
    "mha_images = torch.stack([torch.tensor(mha_images[i]) for i in range(len(mha_images))])\n",
    "th_images = torch.stack([torch.tensor(th_images[i]) for i in range(len(th_images))])\n",
    "mha_images.shape # n_clients * b * 224 * 224\n",
    "th_images.shape # n_clients * b * 224 * 224\n",
    "print(mha_images.shape, th_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_mha, central_th = central_model.module.get_attention_maps_postprocessing_(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_mha = torch.tensor(central_mha)\n",
    "central_mha_agg = []\n",
    "for i in range(central_mha.shape[0]):\n",
    "    central_mha_agg.append(torch.max(central_mha[i], dim=0)[0])\n",
    "central_mha_agg = torch.stack(central_mha_agg)\n",
    "central_mha_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(central_mha_agg[2].cpu())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_dice_scores = []\n",
    "for i in range(10):\n",
    "    central_mha_agg_ = torch.tensor(central_mha_agg[i] > torch.median(central_mha_agg[i])).unsqueeze(0).cpu()\n",
    "    mask_img = masks[i].unsqueeze(0).cpu() > 0\n",
    "    ds = dice_score(central_mha_agg_, mask_img)\n",
    "    mha_dice_scores.append(ds)\n",
    "mha_dice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(masks[0].cpu().permute(1, 2, 0))\n",
    "# plot image : masks > 0\n",
    "plt.imshow(masks[0].cpu().permute(1, 2, 0) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images[0]\n",
    "np.median(grad_cam_images[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mha_images(images, labels, mha_images, pred_labels, central_mha, central_pred_labels):\n",
    "    n_clients, n_images, n_head, h, w = mha_images.shape\n",
    "    image_indices = [2,3,5,8]\n",
    "    row = len(image_indices) * n_head\n",
    "    col = 1 + n_clients + 1\n",
    "    plt.figure(figsize=(3 * col, 3 * row))\n",
    "    for j in range(0, row):\n",
    "        _j = j // n_head\n",
    "        img_index = image_indices[_j]\n",
    "        true_label = labels[img_index]\n",
    "        k = j % n_head\n",
    "        if k == 0:\n",
    "            ax = plt.subplot(row, col, j * col + 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(images[img_index].numpy().transpose(1, 2, 0))\n",
    "            plt.imshow(masks[img_index].numpy().transpose(1, 2, 0), alpha=0.3, cmap='gray')\n",
    "            plt.title(true_label)\n",
    "        for i in range(0, n_clients):\n",
    "            ax = plt.subplot(row, col, j * col + i + 2)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(mha_images[i, img_index, k, :, :].numpy())\n",
    "            plt.imshow(masks[img_index].numpy().transpose(1, 2, 0), alpha=0.3, cmap='gray')\n",
    "            pred_label = pred_labels[i][img_index]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "            if k == 0:\n",
    "                plt.title(f'client {i}: {pred_label}')\n",
    "        ax = plt.subplot(row, col, j * col + n_clients + 2)\n",
    "        plt.imshow(central_mha[img_index, k, :, :])\n",
    "        plt.imshow(masks[img_index].numpy().transpose(1, 2, 0), alpha=0.3, cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        central_pred_label = central_pred_labels[img_index]\n",
    "        border_color = get_border_color(true_label, central_pred_label)\n",
    "        ax.patch.set_edgecolor(border_color)\n",
    "        ax.patch.set_linewidth(7)\n",
    "\n",
    "        if k == 0:\n",
    "            plt.title(f'global: {central_pred_label}, dice: {mha_dice_scores[img_index]:.2f}')\n",
    "    plt.savefig('mha_images.png')\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Usage\n",
    "plot_mha_images(images, labels, mha_images, pred_labels, central_mha, central_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17119ea72eb6b909bd341f4b0d7a48b5939aea29e9bd033254fedca863285074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
