{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch \n",
    "import mymodels \n",
    "import mydataset \n",
    "from torch.utils.data import DataLoader\n",
    "from utils.myfed import *\n",
    "import yaml\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yamlfilepath = pathlib.Path.cwd().parent.joinpath('config.yaml')\n",
    "args = yaml.load(yamlfilepath.open('r'), Loader=yaml.FullLoader)\n",
    "args = argparse.Namespace(**args)\n",
    "args.datapath = \"~/.data\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "\n",
    "# 1. data\n",
    "args.datapath = os.path.expanduser(args.datapath)\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    publicdata = 'cifar100'\n",
    "    args.N_class = 10\n",
    "elif args.dataset == 'cifar100':\n",
    "    publicdata = 'imagenet'\n",
    "    args.N_class = 100\n",
    "elif args.dataset == 'pascal_voc2012':\n",
    "    publicdata = 'mscoco'\n",
    "    args.N_class = 20\n",
    "\n",
    "assert args.dataset in ['cifar10', 'cifar100', 'pascal_voc2012']\n",
    "\n",
    "priv_data, _, test_dataset, public_dataset, distill_loader = mydataset.data_cifar.dirichlet_datasplit(\n",
    "    args, privtype=args.dataset, publictype=publicdata, N_parties=args.N_parties, online=not args.oneshot, public_percent=args.public_percent)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=args.batchsize, shuffle=False, num_workers=args.num_workers, sampler=None)\n",
    "val_loader = DataLoader(\n",
    "    dataset=public_dataset, batch_size=args.batchsize, shuffle=False, num_workers=args.num_workers, sampler=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class)\n",
    "net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "loadname = os.path.join(\"/home/suncheol/code/VFL/FedMAD/checkpoints_backup/pascal_voc2012/a1.0+sd1+e300+b16+lkl\", str(n)+'.pt')\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local{n}......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadname = os.path.join(\"/home/suncheol/code/VFL/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b16+lkl/model-0.pth\")\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local{n}......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "models = []\n",
    "for i in range(0, 5):\n",
    "    model = copy.deepcopy(net)\n",
    "    loadname = os.path.join(f\"/home/suncheol/code/VFL/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b16+lkl/model-{i}.pth\")\n",
    "    if os.path.exists(loadname):\n",
    "        localmodels = torch.load(loadname)\n",
    "        #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "        logging.info(f'Loading Local{n}......', 'filepath : ', loadname)\n",
    "        utils.load_dict(loadname, model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 1 batch of data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels, _ = dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = []\n",
    "for model in models:\n",
    "    grad_cam_images.append(model.module.get_class_activation_map(images, labels))\n",
    "# grayscale_cam = net.module.get_class_activation_map(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = torch.stack([torch.tensor(grad_cam_images[i]) for i in range(len(grad_cam_images))])\n",
    "grad_cam_images.shape # n_clients * b * 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_cam = torch.max(grad_cam_images, dim=0)[0]\n",
    "intersection_cam = torch.min(grad_cam_images, dim=0)[0]\n",
    "union_cam.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 4\n",
    "col = 8\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "for j in range(0, row):\n",
    "    plt.subplot(row, col, j*col+1)\n",
    "    plt.imshow(images[j].numpy().transpose(1, 2, 0))\n",
    "    plt.title(f'original')\n",
    "    for i in range(0, 5):\n",
    "        plt.subplot(row, col, j*col+i+2)\n",
    "        plt.imshow(grad_cam_images[i].numpy()[j])\n",
    "        plt.title(f'client{i}')\n",
    "    plt.subplot(row, col, j*col+7)\n",
    "    plt.imshow(union_cam.numpy()[j])\n",
    "    plt.title('union')\n",
    "    plt.subplot(row, col, j*col+8)\n",
    "    plt.imshow(intersection_cam.numpy()[j])\n",
    "    plt.title('intersection')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale_cam # b * 224 * 224\n",
    "# grayscale_cam = torch.tensor(grayscale_cam)\n",
    "# # n_clients * b * 224 * 224\n",
    "# grayscale_cam = torch.stack([grayscale_cam, grayscale_cam], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale_cam is batch_size x 224 x 224\n",
    "# union is maximum of all CAMs \n",
    "# intersection is minimum of all CAMs\n",
    "union_cam = torch.max(torch.tensor(grayscale_cam), dim=0)[0]\n",
    "intersection_cam = torch.min(torch.tensor(grayscale_cam), dim=0)[0]\n",
    "union_cam.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "mha_images = []\n",
    "for model in models:\n",
    "    mha, thrs = model.module.get_attention_maps_postprocessing(images.cuda())\n",
    "    # mha = model.module.get_attention_maps(images.cuda())[-1]\n",
    "    # print(\"mha shape : \", mha.shape, \"thrs shape : \", thrs.shape)\n",
    "    print(\"mha shape : \", mha.shape)\n",
    "    mha_images.append(mha)\n",
    "\n",
    "print(len(mha_images))\n",
    "mha_images = torch.stack([torch.tensor(mha_images[i]) for i in range(len(mha_images))])\n",
    "print(mha_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = torchvision.utils.make_grid(mha_images[0])\n",
    "torchvision.utils.make_grid(mha_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_images.shape\n",
    "3* 197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mha_images = mha_images.reshape(5, 16, 591, 197).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale_cam is batch_size x 224 x 224\n",
    "# union is maximum of all CAMs \n",
    "# intersection is minimum of all CAMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_images.reshape(5, 3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_cam = torch.max(torch.tensor(mha_images), dim=0)[0]\n",
    "intersection_cam = torch.min(torch.tensor(mha_images), dim=0)[0]\n",
    "union_cam.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = n_heads = 3\n",
    "col = 8\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "for j in range(0, row):\n",
    "    plt.subplot(row, col, j*col+1)\n",
    "    plt.imshow(images[0].numpy().transpose(1, 2, 0))\n",
    "    plt.title(f'original')\n",
    "    for i in range(0, 5):\n",
    "        plt.subplot(row, col, j*col+i+2)\n",
    "        plt.imshow(mha_images[i].numpy()[j])\n",
    "        plt.title(f'client{i}')\n",
    "    plt.subplot(row, col, j*col+7)\n",
    "    plt.imshow(union_cam.numpy()[j])\n",
    "    plt.title('union')\n",
    "    plt.subplot(row, col, j*col+8)\n",
    "    plt.imshow(intersection_cam.numpy()[j])\n",
    "    plt.title('intersection')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_cam = torch.max(mha_images, dim=0)[0]\n",
    "intersection_cam = torch.min(mha_images, dim=0)[0]\n",
    "union_cam.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 4\n",
    "col = 8\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "for j in range(0, row):\n",
    "    plt.subplot(row, col, j*col+1)\n",
    "    plt.imshow(images[j].numpy().transpose(1, 2, 0))\n",
    "    plt.title(f'original')\n",
    "    for i in range(0, 5):\n",
    "        plt.subplot(row, col, j*col+i+2)\n",
    "        plt.imshow(grad_cam_images[i].numpy()[j])\n",
    "        plt.title(f'client{i}')\n",
    "    plt.subplot(row, col, j*col+7)\n",
    "    plt.imshow(union_cam.numpy()[j])\n",
    "    plt.title('union')\n",
    "    plt.subplot(row, col, j*col+8)\n",
    "    plt.imshow(intersection_cam.numpy()[j])\n",
    "    plt.title('intersection')\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import\n",
    "# from __future__ import print_function\n",
    "# from __future__ import division\n",
    "# import math\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class at_loss(torch.nn.Module):\n",
    "#     '''\n",
    "#     summary : FedAD attention loss function\n",
    "#     '''\n",
    "#     def __init__(self): #, T=3, singlelabel=False\n",
    "#         super().__init__()\n",
    "#         # self.T = T\n",
    "#         # self.singlelabel = singlelabel\n",
    "#         # self.criterion= torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "#     def forward(self, inter_input, union_input, target):\n",
    "#         # inter_input : ensembled gradcam image (intersection)\n",
    "#         # union_input : ensembled gradcam image (union)\n",
    "#         # target : central gradcam image\n",
    "#         p1, b1 = 10, 0.6\n",
    "#         p2, b2 = 10, 0.3\n",
    "#         t_A = torch.sigmoid(-p1*(target-b1))\n",
    "#         # Weighted Average sum\n",
    "#         loss1 = - torch.sum(torch.dot(t_A.view(-1), inter_input.view(-1)))/torch.sum(t_A)\n",
    "#         t_U = torch.sigmoid(-p2*(union_input-b2))\n",
    "#         loss2 = - torch.sum(torch.dot(t_U.view(-1), target.view(-1)))/torch.sum(target)\n",
    "#         print('intersection loss : ', loss1, 'union loss : ', loss2)\n",
    "#         return loss1 + loss2\n",
    "\n",
    "\n",
    "# def weight_gradcam(cam_images, countN):#nlcoal*batch*nclass\n",
    "#     #softLogits = torch.nn.Softmax(dim=2)(logits)\n",
    "#     # cam_images = n_clinets * batch size * image width * image height\n",
    "#     # union is maximum of all clients cam_images = batch size * image width * image height\n",
    "#     union = torch.max(torch.tensor(cam_images.clone()), dim=0)[0]\n",
    "#     # intersection is minimum of all clients cam_images = batch size * image width * image height\n",
    "#     intersection = torch.min(torch.tensor(cam_images.clone()), dim=0)[0]\n",
    "#     return union, intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at_loss = at_loss()\n",
    "# union_cam, intersection_cam = weight_gradcam(grayscale_cam, 2)\n",
    "# at_loss(intersection_cam, union_cam, grayscale_cam[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ T(\\bm A) = \\frac {1}{1+exp(-\\rho (\\bm A-b))}. (7) $$\n",
    "\n",
    "$$ \\label {eqinter} \\Loss _\\text {inter}({\\mathbf {\\widetilde A}}, {\\mathbf I}) = - \\frac {1}{C} \\sum _c{{\\frac {\\sum _{hw} {I_{hw}^{c} \\cdot T(\\widetilde {A}_{hw}^c; \\rho _1, b_1)}}{\\sum _{hw} {I_{hw}^{c}}}}}, (8) $$\n",
    "\n",
    "$$ \\label {equnion} \\Loss _\\text {union}({\\mathbf {\\widetilde A}}, {\\mathbf U}) = - \\frac {1}{C} \\sum _c{ {\\frac {\\sum _{hw} {\\widetilde {A}_{hw}^c \\cdot T(U_{hw}^c; \\rho _2, b_2)}}{\\sum _{hw} {\\widetilde {A}_{hw}^c}} }},\n",
    "(9) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(union_cam)\n",
    "# plt.imshow(intersection_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_cam[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(grayscale_cam[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "img_list = []\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(args.batchsize):\n",
    "    np_input = images[i].cpu().numpy()\n",
    "    np_input = np.transpose(np_input, (1, 2, 0))\n",
    "    np_input.shape\n",
    "    grayscale_cam_ = grayscale_cam[i]\n",
    "    cam_image = show_cam_on_image(np_input, grayscale_cam_, use_rgb=True)\n",
    "    img_list.append(cam_image)\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    # plt.imshow(grayscale_cam_)\n",
    "    plt.imshow(cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images and labels\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as pth_transforms\n",
    "# from visualize_attention import company_colors, apply_mask2\n",
    "from PIL import Image, ImageDraw\n",
    "from utils.visualize import * \n",
    "# read image \n",
    "\n",
    "def show_attn(net, img, index=None, nlayer=0):\n",
    "    w_featmap = img.shape[-2] // 16\n",
    "    h_featmap = img.shape[-1] // 16\n",
    "\n",
    "    # attentions = vit.get_last_selfattention(img.cuda())\n",
    "    # attentions = net.module.get_attention_maps(img.cuda())[-1]\n",
    "    attentions = net.module.get_attention_maps(img.cuda())[nlayer]\n",
    "\n",
    "    print('attentions shape', attentions.shape)\n",
    "    print('attentions', attentions)\n",
    "    nh = attentions.shape[1] # number of head\n",
    "    print('number of head', nh)\n",
    "    # we keep only the output patch attention\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "    # we keep only a certain percentage of the mass\n",
    "    val, idx = torch.sort(attentions)\n",
    "    val /= torch.sum(val, dim=1, keepdim=True)\n",
    "    cumval = torch.cumsum(val, dim=1)\n",
    "    th_attn = cumval > (1 - 0.6)\n",
    "    idx2 = torch.argsort(idx)\n",
    "    for head in range(nh):\n",
    "        th_attn[head] = th_attn[head][idx2[head]]\n",
    "    th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "    # interpolate\n",
    "    th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
    "    print('th_attn.shape: ', th_attn.shape)\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=16, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    # save attentions heatmaps\n",
    "    prefix = f'id{index}_' if index is not None else ''\n",
    "    os.makedirs('pics/', exist_ok=True)\n",
    "    torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join('pics/', \"img\" + \".png\"))\n",
    "    img = Image.open(os.path.join('pics/', \"img\" + \".png\"))\n",
    "\n",
    "    attns = Image.new('RGB', (attentions.shape[2] * nh, attentions.shape[1]))\n",
    "    for j in range(nh):\n",
    "        print('attentions[j].shape: ', attentions[j].shape)\n",
    "        fname = os.path.join('pics/', \"attn-head\" + str(j) + \".png\")\n",
    "        plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "        attns.paste(Image.open(fname), (j * attentions.shape[2], 0))\n",
    "\n",
    "    return attentions, th_attn, img, attns\n",
    "\n",
    "\n",
    "# img = Image.open('../data/NIH/processed/images_001/images/00000001_000.png')\n",
    "# img = img.resize((224, 224))\n",
    "img = images.permute(0, 2, 3, 1)[2]\n",
    "img = torch.tensor(np.array(img)).permute(2, 0, 1)\n",
    "img.shape\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.ToPILImage(),\n",
    "    pth_transforms.Grayscale(num_output_channels=3),\n",
    "    pth_transforms.Resize([224, 224]),\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "print(img.shape)\n",
    "# make the image divisible by the patch size\n",
    "w, h = img.shape[-2] - img.shape[-2] % 16, img.shape[-1] - img.shape[-1] % 16\n",
    "print(w, h)\n",
    "img = img[:, :w, :h].unsqueeze(0)\n",
    "print(img.shape)\n",
    "attentions, th_attn, pic_i, pic_attn = show_attn(net, img, nlayer=-1)\n",
    "print(\"attentions.shape: \", attentions.shape)\n",
    "print(\"th_attn.shape: \", th_attn.shape)\n",
    "print(\"pic_i.shape: \", pic_i.size)\n",
    "pic_attn_color = show_attn_color(img[0].permute(1, 2, 0).cpu().numpy(), attentions, th_attn, head=[0,1,2])\n",
    "final_pic = Image.new('RGB', (pic_i.size[1] * 2 + pic_attn.size[0], pic_i.size[1]))\n",
    "final_pic.paste(pic_i, (0, 0))\n",
    "final_pic.paste(pic_attn_color, (pic_i.size[1], 0))\n",
    "final_pic.paste(pic_attn, (pic_i.size[1] * 2, 0))\n",
    "display(final_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(net.module.get_attention_maps(img.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(img.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    attentions = net.module.get_attention_maps(img.cuda())[i]\n",
    "    np_mean = np.mean(attentions.cpu().numpy())\n",
    "    np_std = np.std(attentions.cpu().numpy())\n",
    "    for j in range(attentions.shape[1]):\n",
    "        np_mean = np.mean(attentions[:, j, :, :].cpu().numpy())\n",
    "        np_std = np.std(attentions[:, j, :, :].cpu().numpy())\n",
    "        print(f'layer {i} head {j} mean: {np_mean}, std: {np_std}')\n",
    "    # print(f'layer {i} mean: {np_mean}, std: {np_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "img_list = []\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(args.batchsize):\n",
    "    np_input = images[i].cpu().numpy()\n",
    "    np_input = np.transpose(np_input, (1, 2, 0))\n",
    "    np_input.shape\n",
    "    grayscale_cam_ = grayscale_cam[i]\n",
    "    cam_image = show_cam_on_image(np_input, grayscale_cam_, use_rgb=True)\n",
    "    img_list.append(cam_image)\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(grayscale_cam_)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "565e3544d5dbeb515a1265a05ceb357b4338ebedb8b2db99297d61f63f17eeee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
