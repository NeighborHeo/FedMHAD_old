{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 13 10:50:25 2019\n",
    "\n",
    "@author: Keshik\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision.models as  models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import PascalVOC_Dataset, mydataset\n",
    "import torch.optim as optim\n",
    "from train import train_model, test\n",
    "from utils import encode_labels, plot_history\n",
    "import os\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import utils\n",
    "from calc import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "num = 6\n",
    "# lr = [1e-5, 5e-3] # \n",
    "lr = [1.5e-4, 5e-2]\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "model_name = 'vit_tiny'\n",
    "download_data = False\n",
    "save_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from utils import get_ap_score\n",
    "import numpy as np\n",
    "\n",
    "def test(model, device, test_loader, returnAllScores=False):\n",
    "    \"\"\"\n",
    "    Evaluate a deep neural network model\n",
    "    \n",
    "    Args:\n",
    "        model: pytorch model object\n",
    "        device: cuda or cpu\n",
    "        test_dataloader: test images dataloader\n",
    "        returnAllScores: If true addtionally return all confidence scores and ground truth \n",
    "        \n",
    "    Returns:\n",
    "        test loss and average precision. If returnAllScores = True, check Args\n",
    "    \"\"\"\n",
    "    model.train(False)\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_ap = 0\n",
    "    \n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    m = torch.nn.Sigmoid()\n",
    "    \n",
    "    if returnAllScores == True:\n",
    "        all_scores = np.empty((0, 20), float)\n",
    "        ground_scores = np.empty((0, 20), float)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            #print(data.size(), target.size())\n",
    "            target = target.float()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            bs, ncrops, c, h, w = data.size()\n",
    "\n",
    "            output = model(data.view(-1, c, h, w))\n",
    "            output = output.view(bs, ncrops, -1).mean(1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss # sum up batch loss\n",
    "            running_ap += get_ap_score(torch.Tensor.cpu(target).detach().numpy(), torch.Tensor.cpu(m(output)).detach().numpy()) \n",
    "            \n",
    "            if returnAllScores == True:\n",
    "                all_scores = np.append(all_scores, torch.Tensor.cpu(m(output)).detach().numpy() , axis=0)\n",
    "                ground_scores = np.append(ground_scores, torch.Tensor.cpu(target).detach().numpy() , axis=0)\n",
    "            \n",
    "            del data, target, output\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    num_samples = float(len(test_loader.dataset))\n",
    "    avg_test_loss = running_loss.item()/num_samples\n",
    "    test_map = running_ap/num_samples\n",
    "    \n",
    "    print('test_loss: {:.4f}, test_avg_precision:{:.3f}'.format(\n",
    "                    avg_test_loss, test_map))\n",
    "    \n",
    "    \n",
    "    if returnAllScores == False:\n",
    "        return avg_test_loss, test_map\n",
    "    \n",
    "    return avg_test_loss, test_map, all_scores, ground_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function\n",
    "\n",
    "Args:\n",
    "    data_dir: directory to download Pascal VOC data\n",
    "    model_name: resnet18, resnet34 or resnet50\n",
    "    num: model_num for file management purposes (can be any postive integer. Your results stored will have this number as suffix)\n",
    "    lr: initial learning rate list [lr for resnet_backbone, lr for resnet_fc] \n",
    "    epochs: number of training epochs\n",
    "    batch_size: batch size. Default=16\n",
    "    download_data: Boolean. If true will download the entire 2012 pascal VOC data as tar to the specified data_dir.\n",
    "    Set this to True only the first time you run it, and then set to False. Default False \n",
    "    save_results: Store results (boolean). Default False\n",
    "    \n",
    "Returns:\n",
    "    test-time loss and average precision\n",
    "    \n",
    "Example way of running this function:\n",
    "    if __name__ == '__main__':\n",
    "        main('../data/', \"resnet34\", num=1, lr = [1.5e-4, 5e-2], epochs = 15, batch_size=16, download_data=False, save_results=True)\n",
    "\"\"\"\n",
    "\n",
    "model_dir = os.path.join(\"../models\", model_name)\n",
    "\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir) \n",
    "\n",
    "model_urls = {\n",
    "'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "}\n",
    "\n",
    "from network import define_tsnet\n",
    "\n",
    "model_collections_dict = {\n",
    "        \"resnet18\": models.resnet18(),\n",
    "        \"resnet34\": models.resnet34(),\n",
    "        \"resnet50\": models.resnet50(), \n",
    "        \"vit_tiny\": define_tsnet(\"deit_tiny\", num_class=20, pretrained=True),\n",
    "        }\n",
    "\n",
    "# Initialize cuda parameters\n",
    "use_cuda = torch.cuda.is_available()\n",
    "np.random.seed(2019)\n",
    "torch.manual_seed(2019)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device = \", device)\n",
    "model = model_collections_dict[model_name]\n",
    "model.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "#model.load_state_dict(model_zoo.load_url(model_urls[model_name]))\n",
    "if model_name != \"vit_tiny\":\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(num_ftrs, 20)\n",
    "model.to(device)\n",
    "        \n",
    "optimizer = optim.SGD([   \n",
    "        {'params': list(model.parameters())[:-1], 'lr': lr[0], 'momentum': 0.9},\n",
    "        {'params': list(model.parameters())[-1], 'lr': lr[1], 'momentum': 0.9}\n",
    "        ])\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path('../data/PASCAL_VOC_2012/')\n",
    "val_imgs = np.load(path.joinpath('PASCAL_VOC_val_224_Img.npy'))\n",
    "val_labels = np.load(path.joinpath('PASCAL_VOC_val_224_Label.npy'))\n",
    "train_imgs = np.load(path.joinpath('PASCAL_VOC_train_224_Img.npy'))\n",
    "train_labels = np.load(path.joinpath('PASCAL_VOC_train_224_Label.npy'))\n",
    "\n",
    "val_imgs = np.transpose(val_imgs, (0, 2, 3, 1))*255\n",
    "train_imgs = np.transpose(train_imgs, (0, 2, 3, 1))*255\n",
    "val_imgs = val_imgs.astype(np.uint8)\n",
    "train_imgs = train_imgs.astype(np.uint8)\n",
    "\n",
    "assert(val_imgs.shape[0] == val_labels.shape[0])\n",
    "assert(train_imgs.shape[0] == train_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagnet values\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "transformations = transforms.Compose([transforms.ToPILImage(),\n",
    "                                        # transforms.RandomChoice([\n",
    "                                        #     transforms.ColorJitter(brightness=(0.80, 1.20)),\n",
    "                                        #     transforms.RandomGrayscale(p = 0.25)\n",
    "                                        #     ]),\n",
    "                                        # transforms.RandomHorizontalFlip(p = 0.25),\n",
    "                                        # transforms.RandomRotation(25),\n",
    "                                        transforms.Resize(224), \n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean = mean, std = std),\n",
    "                                    ])\n",
    "transformations_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                        transforms.Resize(224), \n",
    "                                        transforms.ToTensor(), \n",
    "                                        transforms.Normalize(mean = mean, std = std),\n",
    "                                        ])\n",
    "\n",
    "# # model.load_state_dict(torch.load(weights_file_path))\n",
    "transformations_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                        transforms.Resize(224), \n",
    "                                        transforms.FiveCrop(224), \n",
    "                                        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                        transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean = mean, std = std)(crop) for crop in crops])),\n",
    "                                        ])\n",
    "\n",
    "train_dataset = mydataset(train_imgs, train_labels, transforms=transformations)\n",
    "val_dataset = mydataset(val_imgs, val_labels, transforms=transformations_valid)\n",
    "test_dataset = mydataset(val_imgs, val_labels, transforms=transformations_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample images for training\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(train_dataset[i][0].permute(1, 2, 0))\n",
    "    ax[i].set_title(train_dataset[i][1])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (img, labels) in enumerate(train_loader):\n",
    "#     print(img.shape)\n",
    "#     break\n",
    "def test_score(num, save_results=True):\n",
    "    # Load the best weights before testing\n",
    "    weights_file_path = os.path.join(model_dir, \"model-{}.pth\".format(num))\n",
    "    if os.path.isfile(weights_file_path):\n",
    "        print(\"Loading best weights\")\n",
    "        model.load_state_dict(torch.load(weights_file_path))\n",
    "\n",
    "    log_file = open(os.path.join(model_dir, \"log-{}.txt\".format(num)), \"w+\")\n",
    "    log_file.write(\"----------Experiment {} - {}-----------\\n\".format(num, model_name))\n",
    "    log_file.write(\"transformations == {}\\n\".format(transformations.__str__()))\n",
    "    # trn_hist, val_hist = train_model(model, device, optimizer, scheduler, train_loader, val_loader, model_dir, num, epochs, log_file)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # plot_history(trn_hist[0], val_hist[0], \"Loss\", os.path.join(model_dir, \"loss-{}\".format(num)))\n",
    "    # plot_history(trn_hist[1], val_hist[1], \"Accuracy\", os.path.join(model_dir, \"accuracy-{}\".format(num)))    \n",
    "    log_file.close()\n",
    "\n",
    "    #---------------Test your model here---------------------------------------\n",
    "    # Load the best weights before testing\n",
    "    print(\"Evaluating model on test set\")\n",
    "    print(\"Loading best weights\")\n",
    "\n",
    "    if save_results:\n",
    "        loss, ap, scores, gt = test(model, device, test_loader, returnAllScores=True)\n",
    "        # gt_path, scores_path, scores_with_gt_path = os.path.join(model_dir, \"gt-{}.csv\".format(num)), os.path.join(model_dir, \"scores-{}.csv\".format(num)), os.path.join(model_dir, \"scores_wth_gt-{}.csv\".format(num))\n",
    "        # utils.save_results(val_loader.dataset.imgs, gt, utils.object_categories, gt_path)\n",
    "        # utils.save_results(val_loader.dataset.imgs, scores, utils.object_categories, scores_path)\n",
    "        # utils.append_gt(gt_path, scores_path, scores_with_gt_path)\n",
    "        # utils.get_classification_accuracy(gt_path, scores_path, os.path.join(model_dir, \"clf_vs_threshold-{}.png\".format(num)))\n",
    "                \n",
    "        y_score = np.array(scores) \n",
    "        y_test = np.array(gt)\n",
    "        # y_score = np.ones(scores.shape)\n",
    "\n",
    "        th_ls = [0.1 * i for i in range(10)]\n",
    "        opt_th = 0\n",
    "        best_acc = 0\n",
    "        def get_metrics(y_test, y_score, th):\n",
    "            y_pred = (y_score > th).astype(int)\n",
    "            acc = getAccuracy(y_test, y_pred)\n",
    "            pre = getPrecision(y_test, y_pred)\n",
    "            rec = getRecall(y_test, y_pred)\n",
    "            f1 = getF1score(y_test, y_pred)\n",
    "            return acc, pre, rec, f1\n",
    "\n",
    "        for th in th_ls:\n",
    "            acc, pre, rec, f1 = get_metrics(y_test, y_score, th)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                opt_th = th\n",
    "                \n",
    "        acc, pre, rec, f1 = get_metrics(y_test, y_score, opt_th)\n",
    "        # precision round 2\n",
    "        print(\"opt threshold = {0:.2f}\".format(opt_th))\n",
    "        print(\"accuracy = {0:.2f}\".format(acc))\n",
    "        print(\"precision = {0:.2f}\".format(pre))\n",
    "        print(\"recall = {0:.2f}\".format(rec))\n",
    "        print(\"f1 score = {0:.2f}\".format(f1))\n",
    "        print(\"optimal threshold = {}\".format(opt_th), \"best f1 score = {0:.2f}\".format(f1))\n",
    "\n",
    "        figures_dir = os.path.join(os.getcwd(), \"figures\")\n",
    "        if not os.path.exists(figures_dir):\n",
    "            os.makedirs(figures_dir)\n",
    "        plotMultiROCCurve(y_test, y_score, figures_dir, \"roc_curve-{}.png\".format(num))\n",
    "        plotMultilabelconfusionmatrix(y_test, y_score > opt_th, utils.object_categories, figures_dir, \"confusion_matrix-{}.png\".format(num))\n",
    "\n",
    "        performance = {\n",
    "            \"clients\" : num,\n",
    "            \"precision\": f\"{pre:.2f}\",\n",
    "            \"recall\": f\"{rec:.2f}\",\n",
    "            \"f1_score\": f\"{f1:.2f}\",\n",
    "            \"accuracy\": f\"{acc:.2f}\",\n",
    "            \"optimal_threshold\": f\"{opt_th:.2f}\",\n",
    "            \"mAP\": f\"{ap:.2f}\"\n",
    "        }\n",
    "    else:\n",
    "        loss, ap= test(model, device, test_loader, returnAllScores=True)\n",
    "        performance = {}\n",
    "\n",
    "    return performance\n",
    "\n",
    "performance_df = pd.DataFrame()\n",
    "for i in range(0, 5):\n",
    "    performance = test_score(i)\n",
    "    performance_df = pd.concat([performance_df, pd.DataFrame(performance, index=[i])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_df plot line\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "sns.lineplot(data=performance_df, x=\"clients\", y=\"f1_score\", label=\"f1_score\")\n",
    "sns.lineplot(data=performance_df, x=\"clients\", y=\"precision\", label=\"precision\")\n",
    "sns.lineplot(data=performance_df, x=\"clients\", y=\"recall\", label=\"recall\")\n",
    "sns.lineplot(data=performance_df, x=\"clients\", y=\"accuracy\", label=\"accuracy\")\n",
    "sns.lineplot(data=performance_df, x=\"clients\", y=\"mAP\", label=\"mAP\") \n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_precision_score(targets, outputs)\n",
    "\n",
    "# mAP = 0.0\n",
    "# for i in range(len(targets)):\n",
    "#     mAP += average_precision_score(targets[i], outputs[i])\n",
    "# print(mAP/len(targets), len(targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17119ea72eb6b909bd341f4b0d7a48b5939aea29e9bd033254fedca863285074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
