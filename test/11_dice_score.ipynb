{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals().get('__file__') is None:\n",
    "    __file__ = '11_dice_score.ipynb'\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "utils.set_seed(0)\n",
    "def get_unique_classes(dataset):\n",
    "    unique_classes = set()\n",
    "    for img, mask in dataset:\n",
    "        classes = list(mask.getcolors())\n",
    "        for count, pixel_value in classes:\n",
    "            unique_classes.add(pixel_value)\n",
    "    \n",
    "    return unique_classes\n",
    "\n",
    "# 데이터셋 다운로드 및 생성\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224), interpolation=Image.NEAREST), \n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "voc_dataset = VOCSegmentation(root='~/.data', year='2012', image_set='val', download=False, transform=transforms, target_transform=mask_transforms)\n",
    "voc_dataloader = torch.utils.data.DataLoader(voc_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = iter(voc_dataloader).next()\n",
    "labels = utils.masks_to_targets(masks)\n",
    "labels = utils.multi_label_to_multi_captions(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_images(dataset, num_images=5):\n",
    "#     fig, ax = plt.subplots(num_images, 3, figsize=(5, num_images * 2))\n",
    "#     for i in range(num_images):\n",
    "#         # 이미지 및 마스크 가져오기\n",
    "#         img, mask = dataset[i]\n",
    "\n",
    "#         # 이미지 및 마스크 표시\n",
    "#         ax[i, 0].imshow(img.permute(1, 2, 0))\n",
    "#         ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "#         ax[i, 1].imshow(mask.permute(1, 2, 0), cmap='gray')\n",
    "#         ax[i, 1].set_title(f\"Segmentation Image {i + 1}\")\n",
    "#         # overlay segmentation mask on image\n",
    "#         ax[i, 2].imshow(img.permute(1, 2, 0))\n",
    "#         ax[i, 2].imshow(mask.permute(1, 2, 0), alpha=0.4, cmap='gray')\n",
    "#         ax[i, 2].set_title(f\"Overlay {i + 1}\")\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# display_images(voc_dataset)\n",
    "# def display_images(images, masks):\n",
    "#     num_images = len(images)\n",
    "#     print(num_images)\n",
    "#     fig, ax = plt.subplots(num_images, 3, figsize=(5, num_images * 2))\n",
    "#     for i in range(num_images):\n",
    "#         # 이미지 및 마스크 가져오기\n",
    "#         img = images[i].permute(1, 2, 0)\n",
    "#         mask = masks[i].permute(1, 2, 0)\n",
    "\n",
    "#         # 이미지 및 마스크 표시\n",
    "#         ax[i, 0].imshow(img)\n",
    "#         ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "#         ax[i, 1].imshow(mask, cmap='gray')\n",
    "#         ax[i, 1].set_title(f\"Segmentation Image {i + 1}\")\n",
    "#         # overlay segmentation mask on image\n",
    "#         ax[i, 2].imshow(img)\n",
    "#         ax[i, 2].imshow(mask, alpha=0.4, cmap='gray')\n",
    "#         ax[i, 2].set_title(f\"Overlay {i + 1}\")\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('test.png')\n",
    "#     plt.show()\n",
    "    \n",
    "# # 데이터셋에서 이미지 및 마스크 표시\n",
    "# display_images(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch \n",
    "import mymodels \n",
    "import mydataset \n",
    "from torch.utils.data import DataLoader\n",
    "from utils.myfed import *\n",
    "import yaml\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamlfilepath = pathlib.Path.cwd().parent.joinpath('config.yaml')\n",
    "args = yaml.load(yamlfilepath.open('r'), Loader=yaml.FullLoader)\n",
    "args = argparse.Namespace(**args)\n",
    "args.datapath = \"~/.data\"\n",
    "args.batchsize = 200\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "# 1. data\n",
    "args.datapath = os.path.expanduser(args.datapath)\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    publicdata = 'cifar100'\n",
    "    args.N_class = 10\n",
    "elif args.dataset == 'cifar100':\n",
    "    publicdata = 'imagenet'\n",
    "    args.N_class = 100\n",
    "elif args.dataset == 'pascal_voc2012':\n",
    "    publicdata = 'mscoco'\n",
    "    args.N_class = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name = 'vit_tiny_patch16_224'\n",
    "net = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class)\n",
    "net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model1 = copy.deepcopy(net)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h3/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6575.pt', model1)\n",
    "model2 = copy.deepcopy(net)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0/a1.0+sd1+e300+b64+lkl+slNone/oneshot_c1_q0.0_n0.0_h0/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6563.pt', model2)\n",
    "model1 = copy.deepcopy(net)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_only_clean_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h2/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e10_0.5234.pt', model1)\n",
    "model2 = copy.deepcopy(net)\n",
    "# utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_singlelabel/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e20_0.76.pt', model2)\n",
    "utils.load_dict('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_only_clean_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h1/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e10_0.5270.pt', model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dicescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_cam(model, images, labels, th = 0.3):\n",
    "    grad_cam_images = model.module.get_class_activation_map(images, labels)\n",
    "    m = torch.nn.Sigmoid()\n",
    "    th = 0.3\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred_labels = utils.multi_label_to_multi_captions(outputs)\n",
    "    return grad_cam_images, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(y_pred, y_true, smooth=1):\n",
    "    if torch.max(y_pred) > 1:\n",
    "        print(\"y_pred should be in range [0, 1]\")\n",
    "    if torch.max(y_true) > 1:\n",
    "        print(\"y_true should be in range [0, 1]\")\n",
    "    y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "    dice_loss = (2 * (y_pred * y_true).sum() + smooth) / ((y_pred + y_true).sum() + smooth)\n",
    "    return dice_loss\n",
    "\n",
    "def calculate_dice_score(grad_cam_image, masks):\n",
    "    if not isinstance(grad_cam_image, torch.Tensor):\n",
    "        grad_cam_image = torch.tensor(grad_cam_image)\n",
    "    dice_scores = []\n",
    "    for i in range(len(grad_cam_image)):\n",
    "        # print(\"mean, median of grad_cam_image: \", torch.mean(grad_cam_image[i]), torch.median(grad_cam_image[i])) \n",
    "        central_grad_cam = torch.tensor(grad_cam_image[i] > torch.mean(grad_cam_image[i])).float()\n",
    "        mask_img = masks[i].unsqueeze(0).cpu() > 0\n",
    "        ds = dice_score(central_grad_cam, mask_img)\n",
    "        dice_scores.append(ds)\n",
    "    # print(dice_scores)\n",
    "    return dice_scores\n",
    "\n",
    "def getThresholdImages(grad_cam_image):\n",
    "    if not isinstance(grad_cam_image, torch.Tensor):\n",
    "        grad_cam_image = torch.tensor(grad_cam_image)\n",
    "    threshold_images = []\n",
    "    for i in range(len(grad_cam_image)):\n",
    "        image = grad_cam_image[i].clone().detach()\n",
    "        threshold = torch.mean(image)\n",
    "        threshold_image = torch.tensor(image > threshold).float()\n",
    "        threshold_images.append(threshold_image)\n",
    "    return threshold_images\n",
    "\n",
    "def getDiceScore(model, images):\n",
    "    mha, th = model.module.get_attention_maps_postprocessing_(images.cuda())\n",
    "    if not isinstance(mha, torch.Tensor):\n",
    "        mha = torch.tensor(mha)\n",
    "    mha_agg = torch.max(mha, dim=1)[0]\n",
    "    mha_dice_scores = calculate_dice_score(mha_agg, masks)\n",
    "    mha_threshold_images = getThresholdImages(mha_agg)\n",
    "    return {'dice_scores': mha_dice_scores, 'threshold_images': mha_threshold_images, 'mean_dice_score': np.mean(mha_dice_scores)}\n",
    "\n",
    "models_dir_path = '/home/suncheol/code/FedTest/FedMAD/checkpoints/'\n",
    "# search_all_pt_files\n",
    "def find_all_pt_files(path):\n",
    "    pt_file_paths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pt\"):\n",
    "                pt_file_paths.append(os.path.join(root, file))\n",
    "    return pt_file_paths\n",
    "pt_file_paths = find_all_pt_files(models_dir_path)\n",
    "print(pt_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if not os.path.exists(os.path.join(models_dir_path, 'results.csv')):\n",
    "    results = pd.DataFrame(columns=['model_path', 'mean_dice_score'])\n",
    "else:\n",
    "    results = pd.read_csv(os.path.join(models_dir_path, 'results.csv'))\n",
    "    \n",
    "for pt_file_path in pt_file_paths:\n",
    "    if pt_file_path in results['model_path'].values:\n",
    "        continue\n",
    "    print(pt_file_path)\n",
    "    try:\n",
    "        utils.load_dict(pt_file_path, net)\n",
    "        mha_dice_scores = getDiceScore(net, images)\n",
    "        results = results.append({'model_path': pt_file_path, 'mean_dice_score': mha_dice_scores['mean_dice_score']}, ignore_index=True)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        results = results.append({'model_path': pt_file_path, 'mean_dice_score': np.nan}, ignore_index=True)\n",
    "        continue\n",
    "    results.to_csv(os.path.join(models_dir_path, 'results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_cam_images = []\n",
    "# pred_labels = []\n",
    "# for model in models:\n",
    "#     grad_cam_image, pred_label = get_grad_cam(model, images, labels)\n",
    "#     grad_cam_images.append(grad_cam_image)\n",
    "#     pred_labels.append(pred_label)\n",
    "# grad_cam_images = torch.stack([torch.tensor(grad_cam_images[i]) for i in range(len(grad_cam_images))])\n",
    "# grad_cam_images.shape # n_clients * b * 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------\n",
    "# def getAccuracy(y_true, y_pred):\n",
    "#     temp = 0\n",
    "#     for i in range(y_true.shape[0]):\n",
    "#         temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "#     return temp / y_true.shape[0]\n",
    "\n",
    "# def get_Hamming_Loss(y_true, y_pred):\n",
    "#     temp=0\n",
    "#     for i in range(y_true.shape[0]):\n",
    "#         temp += np.size(y_true[i] == y_pred[i]) - np.count_nonzero(y_true[i] == y_pred[i])\n",
    "#     return temp/(y_true.shape[0] * y_true.shape[1])\n",
    "\n",
    "# def getPrecision(y_true, y_pred):\n",
    "#     temp = 0\n",
    "#     for i in range(y_true.shape[0]):\n",
    "#         if sum(y_true[i]) == 0:\n",
    "#             continue\n",
    "#         temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "#     return temp/ y_true.shape[0]\n",
    "\n",
    "# def getRecall(y_true, y_pred):\n",
    "#     temp = 0\n",
    "#     for i in range(y_true.shape[0]):\n",
    "#         if sum(y_pred[i]) == 0:\n",
    "#             continue\n",
    "#         temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "#     return temp/ y_true.shape[0]\n",
    "\n",
    "# def getF1score(y_true, y_pred):\n",
    "#     temp = 0\n",
    "#     for i in range(y_true.shape[0]):\n",
    "#         if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
    "#             continue\n",
    "#         temp+= (2*sum(np.logical_and(y_true[i], y_pred[i])))/ (sum(y_true[i])+sum(y_pred[i]))\n",
    "#     return temp/ y_true.shape[0]\n",
    "\n",
    "# def getMetrics(y_true, y_score, th):\n",
    "#     y_pred = (y_score > th).astype(int)\n",
    "#     acc = getAccuracy(y_true, y_pred)\n",
    "#     pre = getPrecision(y_true, y_pred)\n",
    "#     rec = getRecall(y_true, y_pred)\n",
    "#     f1 = getF1score(y_true, y_pred)\n",
    "#     return acc, pre, rec, f1\n",
    "\n",
    "# def accuracyforsinglelabel(output, target, topk=(1,)):\n",
    "#     output = torch.tensor(output)\n",
    "#     target = torch.tensor(target)\n",
    "#     if len(output.shape) == 2:\n",
    "#         predicted = output.argmax(dim=1)\n",
    "#     if len(target.shape) == 2:\n",
    "#         target = target.argmax(dim=1)\n",
    "    \n",
    "#     total, correct = 0, 0\n",
    "#     total += target.size(0)\n",
    "#     correct += predicted.eq(target).sum().item()\n",
    "#     return correct / total\n",
    "\n",
    "# def accuracy(output, target, topk=(1,)):\n",
    "#     \"\"\"\n",
    "#     usage:\n",
    "#     prec1,prec5=accuracy(output,target,topk=(1,5))\n",
    "#     \"\"\"\n",
    "#     # print(output.shape, target.shape)\n",
    "    \n",
    "#     th_ls = [0.1 * i for i in range(10)]\n",
    "#     opt_th = 0\n",
    "#     best_acc = 0\n",
    "#     for th in th_ls:\n",
    "#         acc, pre, rec, f1 = getMetrics(target, output, th)\n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             opt_th = th\n",
    "\n",
    "#     acc, pre, rec, f1 = getMetrics(target, output, opt_th)\n",
    "#     print(f\"opt_th: {opt_th:.2f}, best_acc: {best_acc:.2f}, pre: {pre:.2f}, rec: {rec:.2f}, f1: {f1:.2f}\")\n",
    "        \n",
    "#     res = []\n",
    "#     for k in topk:\n",
    "#         res.append(acc)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = model1.module.get_class_activation_map(images, labels) \n",
    "grad_cam_image2 = model2.module.get_class_activation_map(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_label(model, images, th=0.3):\n",
    "    m = torch.nn.Sigmoid()\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred = utils.multi_label_to_multi_captions(outputs)\n",
    "    return pred\n",
    "pred_label1 = get_pred_label(model1, images, th=0.3)\n",
    "pred_label2 = get_pred_label(model2, images, th=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images), len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label1 == pred_label2\n",
    "test_score1 = [label == pred for label, pred in zip(labels, pred_label1)]\n",
    "test_score2 = [label == pred for label, pred in zip(labels, pred_label2)]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'test_score1': test_score1, 'test_score2': test_score2})\n",
    "df.apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict = {}\n",
    "for idx, (true_label, pred_label) in enumerate(zip(labels, pred_label1)):\n",
    "    correct_clients_count = client_pred[idx] == true_label for client_pred in pred_labels)\n",
    "    correct_central = pred_label == true_label\n",
    "    correct_dict[idx] = (correct_clients_count, correct_central)\n",
    "\n",
    "# correct_dict to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(correct_dict, orient='index', columns=['correct_clients_count', 'correct_central'])\n",
    "df = df.pivot_table(index='correct_clients_count', columns='correct_central', aggfunc=len, fill_value=0)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dice_scores = calculate_dice_score(grad_cam_image, masks)\n",
    "dice_scores2 = calculate_dice_score(grad_cam_image2, masks)\n",
    "print(\"mean of dice score: \", torch.mean(torch.tensor(dice_scores)), torch.mean(torch.tensor(dice_scores2)))\n",
    "\n",
    "\n",
    "threshold_images = getThresholdImages(grad_cam_image)\n",
    "threshold_images2 = getThresholdImages(grad_cam_image2)\n",
    "\n",
    "def drawplots(images, masks, grad_cam_images, threshold_images):\n",
    "    length = len(images)\n",
    "    fig, ax = plt.subplots(length, 4, figsize=(20, 20))\n",
    "    for i in range(length):\n",
    "        ax[i, 0].imshow(images[i].permute(1, 2, 0))\n",
    "        ax[i, 0].set_title(f\"Image {i + 1}\")\n",
    "        ax[i, 1].imshow(masks[i].permute(1, 2, 0), alpha=0.4, cmap='gray')\n",
    "        ax[i, 1].set_title(f\"Mask {i + 1}\")\n",
    "        ax[i, 2].imshow(grad_cam_images[i])\n",
    "        ax[i, 2].set_title(f\"Grad CAM {i + 1}\")\n",
    "        ax[i, 3].imshow(threshold_images[i], alpha=0.4, cmap='gray')\n",
    "        ax[i, 3].set_title(f\"Threshold {i + 1}\")\n",
    "    plt.show()\n",
    "# drawplots(images, masks, grad_cam_image, threshold_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawplots(images, masks, mha_agg, mha_threshold_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "loadname = os.path.join(\"/home/suncheol/code/VFL/FedMAD/checkpoints_backup/pascal_voc2012/a1.0+sd1+e300+b16+lkl\", str(n)+'.pt')\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local{n}......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadname = os.path.join(\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b16+lkl/model-0.pth\")\n",
    "loadname = os.path.join(\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b128+lkl+slmha/oneshot_c1_q0.0_n0.0/q0.0_n0.0_ADAM_b128_5e-05_200_5e-05_m0.9_e10_0.66.pt\")\n",
    "# loadname = os.path.join(\"/home/suncheol/code/FedTest/pytorch-models/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_-1/ckpt.pth\")\n",
    "if os.path.exists(loadname):\n",
    "    localmodels = torch.load(loadname)\n",
    "    #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "    logging.info(f'Loading Local......')\n",
    "    print('filepath : ', loadname)\n",
    "    utils.load_dict(loadname, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "models = []\n",
    "for i in range(0, 5):\n",
    "    model = copy.deepcopy(net)\n",
    "    loadname = os.path.join(f\"/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/a1.0+sd1+e300+b128+lkl+slmha/model-{i}.pth\")\n",
    "    # loadname = os.path.join(f\"/home/suncheol/code/FedTest/pytorch-models/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_{i}/ckpt.pth\")\n",
    "    if os.path.exists(loadname):\n",
    "        localmodels = torch.load(loadname)\n",
    "        #self.localmodels[n].load_state_dict(self.best_statdict, strict=True)\n",
    "        logging.info(f'Loading Local......', 'filepath : ', loadname)\n",
    "        utils.load_dict(loadname, model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = utils.multi_label_to_multi_captions(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images = []\n",
    "pred_labels = []\n",
    "for model in models:\n",
    "    grad_cam_images.append(model.module.get_class_activation_map(images, labels))\n",
    "    m = torch.nn.Sigmoid()\n",
    "    th = 0.3\n",
    "    outputs = m(model(images)).detach().cpu().numpy()\n",
    "    outputs[outputs > th] = 1\n",
    "    outputs[outputs <= th] = 0\n",
    "    pred = multi_label_to_multi_captions(outputs)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "grad_cam_images = torch.stack([torch.tensor(grad_cam_images[i]) for i in range(len(grad_cam_images))])\n",
    "grad_cam_images.shape # n_clients * b * 224 * 224\n",
    "\n",
    "# grayscale_cam = net.module.get_class_activation_map(images, labels)\n",
    "central_model = copy.deepcopy(net)\n",
    "central_grad_cam_image = central_model.module.get_class_activation_map(images, labels)\n",
    "m = torch.nn.Sigmoid()\n",
    "th = 0.3\n",
    "outputs = m(central_model(images)).detach().cpu().numpy()\n",
    "outputs[outputs > th] = 1\n",
    "outputs[outputs <= th] = 0\n",
    "central_pred_labels = multi_label_to_multi_captions(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_cam = torch.max(grad_cam_images, dim=0)[0]\n",
    "intersection_cam = torch.min(grad_cam_images, dim=0)[0]\n",
    "union_cam.cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_border_color(true_label, pred_label):\n",
    "    if pred_label == true_label:\n",
    "        return 'lime'  # green\n",
    "    elif set(pred_label) & set(true_label):\n",
    "        return 'gold'  # yellow\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "row = 4\n",
    "col = 9\n",
    "clients = 5\n",
    "extra_plots = 3  # union, intersection, global\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "\n",
    "for j in range(row):\n",
    "    true_label = labels[j]\n",
    "    for i in range(clients + extra_plots + 1):\n",
    "        ax = plt.subplot(row, col, j * col + i + 1)\n",
    "        # print(i, j)\n",
    "        if i == 0:\n",
    "            img = images[j].cpu().permute(1, 2, 0)\n",
    "            mask = masks[j].permute(1, 2, 0).cpu()\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'GT: {true_label}')\n",
    "        elif i <= clients:\n",
    "            img = grad_cam_images[i - 1].cpu()[j]\n",
    "            pred_label = pred_labels[i - 1][j]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            # set border color\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(5)\n",
    "            plt.title(f'client{i}: {pred_label}')\n",
    "        elif i == clients + 1:\n",
    "            img = union_cam.cpu()[j]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('union')\n",
    "        elif i == clients + 2:\n",
    "            img = intersection_cam.cpu()[j]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('intersection')\n",
    "        elif i == clients + 3:\n",
    "            img = central_grad_cam_image[j]\n",
    "            border_color = get_border_color(true_label, central_pred_labels[j])\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'global: {central_pred_labels[j]}')\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(5)\n",
    "        # plt.gca().set_xticks([])\n",
    "        # plt.gca().set_yticks([])\n",
    "        # plt.gca().spines['top'].set_visible(False)\n",
    "        # plt.gca().spines['right'].set_visible(False)\n",
    "        # plt.gca().spines['bottom'].set_visible(False)\n",
    "        # plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.savefig('grad_cam.png')\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict = {}\n",
    "for idx, (true_label, central_pred) in enumerate(zip(labels, central_pred_labels)):\n",
    "    correct_clients_count = sum(client_pred[idx] == true_label for client_pred in pred_labels)\n",
    "    correct_central = central_pred == true_label\n",
    "    correct_dict[idx] = (correct_clients_count, correct_central)\n",
    "\n",
    "# correct_dict to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(correct_dict, orient='index', columns=['correct_clients_count', 'correct_central'])\n",
    "df = df.pivot_table(index='correct_clients_count', columns='correct_central', aggfunc=len, fill_value=0)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wrong_clients_correct_central(true_labels, client_pred_labels, central_pred_labels):\n",
    "    result_indices = []\n",
    "    for idx, (true_label, central_pred) in enumerate(zip(true_labels, central_pred_labels)):\n",
    "        if sum(client_pred[idx] == true_label for client_pred in client_pred_labels) < 2 and central_pred == true_label:\n",
    "            result_indices.append(idx)\n",
    "    return result_indices\n",
    "\n",
    "def find_correct_clients_wrong_central(true_labels, client_pred_labels, central_pred_labels):\n",
    "    result_indices = []\n",
    "    for idx, (true_label, central_pred) in enumerate(zip(true_labels, central_pred_labels)):\n",
    "        # if all(client_pred[idx] == true_label for client_pred in client_pred_labels) and central_pred != true_label:\n",
    "        #     result_indices.append(idx)\n",
    "        # more than 3 clients are correct and central is wrong\n",
    "        if sum(client_pred[idx] == true_label for client_pred in client_pred_labels) > 3 and central_pred != true_label:\n",
    "            result_indices.append(idx)\n",
    "            \n",
    "    return result_indices\n",
    "\n",
    "# Example usage\n",
    "wrong_clients_correct_central_indices = find_wrong_clients_correct_central(labels, pred_labels, central_pred_labels[0])\n",
    "print(\"Wrong clients, correct central indices:\", wrong_clients_correct_central_indices)\n",
    "\n",
    "correct_clients_wrong_central_indices = find_correct_clients_wrong_central(labels, pred_labels, central_pred_labels[0])\n",
    "print(\"Correct clients, wrong central indices:\", correct_clients_wrong_central_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(y_pred, y_true, smooth=1):\n",
    "    if torch.max(y_pred) > 1:\n",
    "        print(\"y_pred should be in range [0, 1]\")\n",
    "    if torch.max(y_true) > 1:\n",
    "        print(\"y_true should be in range [0, 1]\")\n",
    "    y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "    dice_loss = (2 * (y_pred * y_true).sum() + smooth) / ((y_pred + y_true).sum() + smooth)\n",
    "    return dice_loss\n",
    "\n",
    "dice_scores = []\n",
    "for i in range(10):\n",
    "    central_grad_cam = torch.tensor(central_grad_cam_image[i] >0.1).unsqueeze(0).cpu()\n",
    "    mask_img = masks[i].unsqueeze(0).cpu() > 0\n",
    "    ds = dice_score(central_grad_cam, mask_img)\n",
    "    dice_scores.append(ds)\n",
    "\n",
    "print(dice_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_border_color(true_label, pred_label):\n",
    "    if pred_label == true_label:\n",
    "        return 'lime'  # green\n",
    "    elif set(pred_label) & set(true_label):\n",
    "        return 'gold'  # yellow\n",
    "    else:\n",
    "        return 'red'\n",
    "image_index = [1, 4, 5, 8]\n",
    "row = len(image_index)\n",
    "col = 9\n",
    "clients = 5\n",
    "extra_plots = 3  # union, intersection, global\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3 * col, 3 * row))\n",
    "\n",
    "for j in range(row):\n",
    "    idxImage = image_index[j]\n",
    "    true_label = labels[idxImage]\n",
    "    for i in range(clients + extra_plots + 1):\n",
    "        ax = plt.subplot(row, col, j * col + i + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # print(i, idxImage)\n",
    "        if i == 0:\n",
    "            img = images[idxImage].cpu().permute(1, 2, 0)\n",
    "            mask = masks[idxImage].cpu().permute(1, 2, 0)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'GT: {true_label}')\n",
    "        elif i <= clients:\n",
    "            img = grad_cam_images[i - 1].cpu()[idxImage]\n",
    "            pred_label = pred_labels[i - 1][idxImage]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            # set border color\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "            plt.title(f'client{i}: {pred_label}')\n",
    "        elif i == clients + 1:\n",
    "            img = union_cam.cpu()[idxImage]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('union')\n",
    "        elif i == clients + 2:\n",
    "            img = intersection_cam.cpu()[idxImage]\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title('intersection')\n",
    "        elif i == clients + 3:\n",
    "            img = central_grad_cam_image[idxImage]\n",
    "            border_color = get_border_color(true_label, central_pred_labels[idxImage])\n",
    "            plt.imshow(img)\n",
    "            plt.imshow(mask, alpha=0.3, cmap='gray')\n",
    "            plt.title(f'global: {central_pred_labels[idxImage]}, dice: {dice_scores[idxImage]:.2f}')\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "        # plt.gca().set_xticks([])\n",
    "        # plt.gca().set_yticks([])\n",
    "        # plt.gca().spines['top'].set_visible(False)\n",
    "        # plt.gca().spines['right'].set_visible(False)\n",
    "        # plt.gca().spines['bottom'].set_visible(False)\n",
    "        # plt.gca().spines['left'].set_visible(False)\n",
    "\n",
    "plt.savefig('grad_cam2.png')\n",
    "plt.show()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_images = [] \n",
    "th_images = []\n",
    "for model in models:\n",
    "    mha, th = model.module.get_attention_maps_postprocessing_(images.cuda())\n",
    "    mha_images.append(mha)\n",
    "    th_images.append(th)\n",
    "    \n",
    "mha_images = torch.stack([torch.tensor(mha_images[i]) for i in range(len(mha_images))])\n",
    "th_images = torch.stack([torch.tensor(th_images[i]) for i in range(len(th_images))])\n",
    "mha_images.shape # n_clients * b * 224 * 224\n",
    "th_images.shape # n_clients * b * 224 * 224\n",
    "print(mha_images.shape, th_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_mha, central_th = central_model.module.get_attention_maps_postprocessing_(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_mha = torch.tensor(central_mha)\n",
    "central_mha_agg = []\n",
    "for i in range(central_mha.shape[0]):\n",
    "    central_mha_agg.append(torch.max(central_mha[i], dim=0)[0])\n",
    "central_mha_agg = torch.stack(central_mha_agg)\n",
    "central_mha_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(central_mha_agg[2].cpu())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_dice_scores = []\n",
    "for i in range(10):\n",
    "    central_mha_agg_ = torch.tensor(central_mha_agg[i] > torch.median(central_mha_agg[i])).unsqueeze(0).cpu()\n",
    "    mask_img = masks[i].unsqueeze(0).cpu() > 0\n",
    "    ds = dice_score(central_mha_agg_, mask_img)\n",
    "    mha_dice_scores.append(ds)\n",
    "mha_dice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(masks[0].cpu().permute(1, 2, 0))\n",
    "# plot image : masks > 0\n",
    "plt.imshow(masks[0].cpu().permute(1, 2, 0) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images[0]\n",
    "np.median(grad_cam_images[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mha_images(images, labels, mha_images, pred_labels, central_mha, central_pred_labels):\n",
    "    n_clients, n_images, n_head, h, w = mha_images.shape\n",
    "    image_indices = [2,3,5,8]\n",
    "    row = len(image_indices) * n_head\n",
    "    col = 1 + n_clients + 1\n",
    "    plt.figure(figsize=(3 * col, 3 * row))\n",
    "    for j in range(0, row):\n",
    "        _j = j // n_head\n",
    "        img_index = image_indices[_j]\n",
    "        true_label = labels[img_index]\n",
    "        k = j % n_head\n",
    "        if k == 0:\n",
    "            ax = plt.subplot(row, col, j * col + 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(images[img_index].numpy().transpose(1, 2, 0))\n",
    "            plt.imshow(masks[img_index].numpy().transpose(1, 2, 0), alpha=0.3, cmap='gray')\n",
    "            plt.title(true_label)\n",
    "        for i in range(0, n_clients):\n",
    "            ax = plt.subplot(row, col, j * col + i + 2)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plt.imshow(mha_images[i, img_index, k, :, :].numpy())\n",
    "            plt.imshow(masks[img_index].numpy().transpose(1, 2, 0), alpha=0.3, cmap='gray')\n",
    "            pred_label = pred_labels[i][img_index]\n",
    "            border_color = get_border_color(true_label, pred_label)\n",
    "            ax.patch.set_edgecolor(border_color)\n",
    "            ax.patch.set_linewidth(7)\n",
    "            if k == 0:\n",
    "                plt.title(f'client {i}: {pred_label}')\n",
    "        ax = plt.subplot(row, col, j * col + n_clients + 2)\n",
    "        plt.imshow(central_mha[img_index, k, :, :])\n",
    "        plt.imshow(masks[img_index].numpy().transpose(1, 2, 0), alpha=0.3, cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        central_pred_label = central_pred_labels[img_index]\n",
    "        border_color = get_border_color(true_label, central_pred_label)\n",
    "        ax.patch.set_edgecolor(border_color)\n",
    "        ax.patch.set_linewidth(7)\n",
    "\n",
    "        if k == 0:\n",
    "            plt.title(f'global: {central_pred_label}, dice: {mha_dice_scores[img_index]:.2f}')\n",
    "    plt.savefig('mha_images.png')\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Usage\n",
    "plot_mha_images(images, labels, mha_images, pred_labels, central_mha, central_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17119ea72eb6b909bd341f4b0d7a48b5939aea29e9bd033254fedca863285074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
