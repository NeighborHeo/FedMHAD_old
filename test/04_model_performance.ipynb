{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import utils\n",
    "\n",
    "import pathlib\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch \n",
    "import mymodels \n",
    "import mydataset \n",
    "from torch.utils.data import DataLoader\n",
    "from utils.myfed import *\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __file__ is not defined:\n",
    "if globals().get('__file__') is None:\n",
    "    __file__ = '04_model_performance.ipynb'\n",
    "\n",
    "parent_path = pathlib.Path(\"../\").resolve()\n",
    "yamlfilepath = parent_path.joinpath('config.yaml')\n",
    "args = yaml.load(yamlfilepath.open('r'), Loader=yaml.FullLoader)\n",
    "args = argparse.Namespace(**args)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    publicdata = 'cifar100'\n",
    "    args.N_class = 10\n",
    "elif args.dataset == 'cifar100':\n",
    "    publicdata = 'imagenet'\n",
    "    args.N_class = 100\n",
    "elif args.dataset == 'pascal_voc2012':\n",
    "    publicdata = 'mscoco'\n",
    "    args.N_class = 20\n",
    "\n",
    "assert args.dataset in ['cifar10', 'cifar100', 'pascal_voc2012']\n",
    "args.datapath = os.path.expanduser(args.datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priv_data, _, test_dataset, public_dataset, distill_loader = mydataset.data_cifar.dirichlet_datasplit(\n",
    "    args, privtype=args.dataset, publictype=publicdata, N_parties=args.N_parties, online=not args.oneshot, public_percent=args.public_percent)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=args.batchsize, shuffle=False, num_workers=args.num_workers, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_label_counts = 0\n",
    "# for i in range(target.shape[0]):\n",
    "#     label_counts = np.sum(target[i])\n",
    "#     if label_counts > max_label_counts:\n",
    "#         max_label_counts = label_counts\n",
    "# print(max_label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    # testacc = utils.AverageMeter()\n",
    "    m = torch.nn.Sigmoid()\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(test_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "            output = model(images)\n",
    "            \n",
    "            output_list.append(m(output).detach().cpu().numpy())\n",
    "            target_list.append(target.detach().cpu().numpy())\n",
    "            \n",
    "            # testacc.update(acc)\n",
    "    output = np.concatenate(output_list, axis=0)\n",
    "    target = np.concatenate(target_list, axis=0)\n",
    "    acc, = utils.accuracy(output, target)\n",
    "    top_k = utils.multi_label_top_margin_k_accuracy(target, output, margin=0)\n",
    "    mAP, _ = utils.compute_mean_average_precision(target, output)\n",
    "    acc, top_k, mAP = round(acc, 4), round(top_k, 4), round(mAP, 4)\n",
    "    print(f'Acc: {acc}, Top-k: {top_k}, mAP: {mAP}')\n",
    "    return {'acc': acc, 'top_k': top_k, 'mAP': mAP}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['acc', 'top_k', 'mAP'])\n",
    "for i in range(5):\n",
    "    model = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class, pretrained=args.pretrained)\n",
    "    # load_model = torch.load('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h3/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6575.pt')\n",
    "    utils.load_dict(f'/home/suncheol/code/FedTest/FED_MHAD_sub/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0_2e-05/model-{i}.pth', model)\n",
    "    print(\"i : \", i)\n",
    "    result = test(model, test_loader)\n",
    "    df.loc[i] = result\n",
    "df.to_csv(f'/home/suncheol/code/FedTest/FED_MHAD_sub/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0_2e-05/result.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name = 'vit_tiny_patch16_224'\n",
    "model = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class, pretrained=args.pretrained)\n",
    "# model.module.setExcludedHead([0])\n",
    "# load_model = torch.load('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h3/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6575.pt')\n",
    "utils.load_dict(f'/home/suncheol/code/FedTest/FED_MHAD/test/04_pascal_voc_fed_avg_multilabel/checkpoints/27197_server_best_models/model_round100_acc0.76_loss0.00.pth', model)\n",
    "# model.\n",
    "# print(\"i : \", i)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class, pretrained=args.pretrained)\n",
    "# load_model = torch.load('/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h3/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e7_0.6575.pt')\n",
    "load_dict(f'/home/suncheol/code/FedTest/pytorch-model-multiclass/checkpoint/pascal_voc_vit_tiny_patch16_224_0.0001_-1_multilabel/ckpt.pth', model)\n",
    "print(\"i : \", i)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find .pt files in dir\n",
    "dir_path = \"/home/suncheol/code/FedTest/FED_MHAD_sub/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0_2e-05/a1.0+sd1+e300+b64+lkl+slmha\"\n",
    "# search .pt files in under dir\n",
    "file_list = os.listdir(dir_path)\n",
    "pt_file_list = [file for file in file_list if file.endswith(\".pth\")]\n",
    "print(pt_file_list)\n",
    "df = pd.DataFrame(columns=['acc', 'top_k', 'mAP'])\n",
    "for i in range(len(pt_file_list)):\n",
    "    model = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class, pretrained=args.pretrained)\n",
    "    load_dict(dir_path + '/' + pt_file_list[i], model)\n",
    "    print(\"i : \", i)\n",
    "    result = test(model, test_loader)\n",
    "    df.loc[i] = result\n",
    "df.to_csv(dir_path + '/result.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodels.define_model(modelname=args.model_name, num_classes=args.N_class, pretrained=args.pretrained)\n",
    "load_dict(f'/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_noisy_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h2/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e3_0.5793.pt', model)\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dict(f'/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_noisy_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h1/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e11_0.5782.pt', model)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dict(f'/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h1/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e10_0.6554.pt', model)\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dict(f'/home/suncheol/code/FedTest/FedMAD/checkpoints/pascal_voc2012/vit_tiny_patch16_224_multilabel_clean_1.0/a1.0+sd1+e300+b64+lkl+slmha/oneshot_c1_q0.0_n0.0_h2/q0.0_n0.0_ADAM_b64_2e-05_200_1e-05_m0.9_e10_0.6544.pt', model)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                     'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "                     'cow', 'diningtable', 'dog', 'horse',\n",
    "                     'motorbike', 'person', 'pottedplant',\n",
    "                     'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "y_score = np.array(output) \n",
    "y_test = np.array(target)\n",
    "\n",
    "th_ls = [0.1 * i for i in range(10)]\n",
    "opt_th = 0\n",
    "best_acc = 0\n",
    "def get_metrics(y_test, y_score, th):\n",
    "    y_pred = (y_score > th).astype(int)\n",
    "    acc = getAccuracy(y_test, y_pred)\n",
    "    pre = getPrecision(y_test, y_pred)\n",
    "    rec = getRecall(y_test, y_pred)\n",
    "    f1 = getF1score(y_test, y_pred)\n",
    "    return acc, pre, rec, f1\n",
    "\n",
    "for th in th_ls:\n",
    "    acc, pre, rec, f1 = get_metrics(y_test, y_score, th)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        opt_th = th\n",
    "        \n",
    "acc, pre, rec, f1 = get_metrics(y_test, y_score, opt_th)\n",
    "print(\"opt threshold = {}\".format(opt_th))\n",
    "print(\"accuracy = {}\".format(acc))\n",
    "print(\"precision = {}\".format(pre))\n",
    "print(\"recall = {}\".format(rec))\n",
    "print(\"f1 score = {}\".format(f1))\n",
    "print(\"optimal threshold = {}\".format(opt_th), \"best f1 score = {}\".format(best_acc))\n",
    "\n",
    "plotMultiROCCurve(y_test, y_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_cm = multi_label_confusion_matrix(y_test, y_score > opt_th, object_categories)\n",
    "print(ml_cm)\n",
    "plotMultilabelconfusionmatrix(y_test, y_score > opt_th, object_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.multilabel_confusion_matrix(y_test, y_score > opt_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_score > opt_th).astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "labels = object_categories\n",
    "cm = metrics.multilabel_confusion_matrix(y_test, y_pred)\n",
    "''' plot n * 4 subplots '''\n",
    "nClasses = len(labels)\n",
    "fig, ax = plt.subplots(int(nClasses/5), 5, figsize=(10, 8))\n",
    "for axes, cfs_matrix, label in zip(ax.flatten(), cm, labels):\n",
    "    print(label)\n",
    "    df_cm = pd.DataFrame(cfs_matrix, index = [i for i in [\"True\", \"False\"]],\n",
    "                columns = [i for i in [\"True\", \"False\"]])\n",
    "    sns.heatmap(df_cm, annot=True, ax = axes, fmt='g')\n",
    "    axes.set_title(label)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17119ea72eb6b909bd341f4b0d7a48b5939aea29e9bd033254fedca863285074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
