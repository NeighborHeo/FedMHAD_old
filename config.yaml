debug: False # help="debug mode, only run 1 epoch"
gpu: '0' # help="gpu id"
num_workers: 8 # help="num of workers"
model_name: vit_tiny_patch16_224 # help="model name"
pretrained: True # help="pretrained model"
dataset: pascal_voc2012 # help="dataset name"
datapath: ~/.data/ # help="dataset path" 
logfile:  ''# help="log file path"
subpath:  ''# help="subpath under localtraining folder to save central model"
N_parties: 5 # help="number of parties"
alpha: 1.0 # help="alpha"
seed: 1 # help="seed"
C: 1 # help="percent of locals selected in each fed communication round"
fedinitepochs: 20 # help="epochs of local training"
batchsize: 64 # help="batch size"
lr: 0.00002 # 0.0005  # help="learning rate"
lrmin: 0.00001 # help="learning rate min"
distill_droprate: 0 # help="distill droprate"
fedrounds: 200 # help="fed rounds"
public_percent: 1.0 # help="ablation for c100 as public data"
oneshot: False # help="only valid when wo/ QN"
joint: False # help="only valid when wo/ QN"
quantify: 0.0 # help="when w/ QN"
noisescale: 0.0 # help="when w/ QN"
disbatchsize: 64 # help="batch size for discriminator" # 512
localepochs: 10 # help="epochs of local training"
initepochs: 300  # help="epochs of local training"
initcentral:  # help="ckpt used to init central model, import for co-distillation"
steps_round: 10000  # help="steps per round"
dis_lr: 0.00002 # help="learning rate"
dis_lrmin: 0.00001  # help="learning rate min"
optim: ADAM # help="optimizer"
momentum: 0.9 # help="momentum"
wdecay: 0.0 # help="weight decay"
voteout: False  # help="vote out"
clscnt: 1 # help="local weight specific to class"
lossmode: kl # help="kl or l1"
sublossmode: mha # mha # help="at or mha"
singlelabel: False # help="single label"
task: multilabel # help="task name"
distill_heads: 3 # help="distill heads"
lambda_kd: 0.1 # help="lambda kd"